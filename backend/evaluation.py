#!/usr/bin/env python3
"""
RAG ì‹œìŠ¤í…œ í‰ê°€ ëª¨ë“ˆ
ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ RAG íŒŒì´í”„ë¼ì¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ í•µì‹¬ ì§€í‘œë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
"""

from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity


@dataclass
class EvaluationResult:
    """RAG í‰ê°€ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    context_relevancy_score: float
    faithfulness_score: float
    answer_relevancy_score: float
    overall_score: float
    details: Dict[str, Any]


class RAGEvaluator:
    """
    RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤
    
    í•µì‹¬ í‰ê°€ ì§€í‘œ:
    1. Context Relevancy: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ê°€?
    2. Faithfulness: ìƒì„±ëœ ë‹µë³€ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œê°€?
    3. Answer Relevancy: ìƒì„±ëœ ë‹µë³€ì´ ì›ë³¸ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ê°€?
    """
    
    def __init__(self):
        """í‰ê°€ ëª¨ë¸ ì´ˆê¸°í™”"""
        self.similarity_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    
    def evaluate_context_relevancy(self, query: str, retrieved_contexts: List[str]) -> float:
        """
        Context Relevancy í‰ê°€
        
        ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •í•©ë‹ˆë‹¤.
        ë†’ì€ ì ìˆ˜ëŠ” ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ë” ê´€ë ¨ì„±ì´ ë†’ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            retrieved_contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            float: 0.0 ~ 1.0 ì‚¬ì´ì˜ ê´€ë ¨ì„± ì ìˆ˜
        """
        if not retrieved_contexts:
            return 0.0
        
        # ì¿¼ë¦¬ì™€ ê° ì»¨í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        query_embedding = self.similarity_model.encode([query])
        context_embeddings = self.similarity_model.encode(retrieved_contexts)
        
        similarities = cosine_similarity(query_embedding, context_embeddings)[0]
        
        # í‰ê·  ìœ ì‚¬ë„ ë°˜í™˜
        return float(np.mean(similarities))
    
    def evaluate_faithfulness(self, generated_answer: str, retrieved_contexts: List[str]) -> float:
        """
        Faithfulness í‰ê°€
        
        ìƒì„±ëœ ë‹µë³€ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œì§€ ì¸¡ì •í•©ë‹ˆë‹¤.
        ë†’ì€ ì ìˆ˜ëŠ” ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ì— ë” ì¶©ì‹¤í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
        
        Args:
            generated_answer: ìƒì„±ëœ ë‹µë³€
            retrieved_contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            float: 0.0 ~ 1.0 ì‚¬ì´ì˜ ì¶©ì‹¤ë„ ì ìˆ˜
        """
        if not retrieved_contexts:
            return 0.0
        
        # ë‹µë³€ê³¼ ê° ì»¨í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        answer_embedding = self.similarity_model.encode([generated_answer])
        context_embeddings = self.similarity_model.encode(retrieved_contexts)
        
        similarities = cosine_similarity(answer_embedding, context_embeddings)[0]
        
        # ìµœëŒ€ ìœ ì‚¬ë„ ë°˜í™˜ (ë‹µë³€ì´ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œê°€?)
        return float(np.max(similarities))
    
    def evaluate_answer_relevancy(self, query: str, generated_answer: str) -> float:
        """
        Answer Relevancy í‰ê°€
        
        ìƒì„±ëœ ë‹µë³€ì´ ì›ë³¸ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •í•©ë‹ˆë‹¤.
        ë†’ì€ ì ìˆ˜ëŠ” ë‹µë³€ì´ ì§ˆë¬¸ì— ë” ì ì ˆí•˜ê²Œ ì‘ë‹µí•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            generated_answer: ìƒì„±ëœ ë‹µë³€
            
        Returns:
            float: 0.0 ~ 1.0 ì‚¬ì´ì˜ ê´€ë ¨ì„± ì ìˆ˜
        """
        # ì§ˆë¬¸ê³¼ ë‹µë³€ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        query_embedding = self.similarity_model.encode([query])
        answer_embedding = self.similarity_model.encode([generated_answer])
        
        similarity = cosine_similarity(query_embedding, answer_embedding)[0][0]
        return float(similarity)
    
    def evaluate_rag_system(self, 
                           query: str, 
                           retrieved_contexts: List[str], 
                           generated_answer: str) -> EvaluationResult:
        """
        RAG ì‹œìŠ¤í…œì˜ ì¢…í•©ì ì¸ ì„±ëŠ¥ í‰ê°€
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            retrieved_contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            generated_answer: ìƒì„±ëœ ë‹µë³€
            
        Returns:
            EvaluationResult: í‰ê°€ ê²°ê³¼ ê°ì²´
        """
        # ê° ì§€í‘œë³„ ì ìˆ˜ ê³„ì‚°
        context_relevancy = self.evaluate_context_relevancy(query, retrieved_contexts)
        faithfulness = self.evaluate_faithfulness(generated_answer, retrieved_contexts)
        answer_relevancy = self.evaluate_answer_relevancy(query, generated_answer)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        overall_score = (context_relevancy * 0.3 + 
                        faithfulness * 0.4 + 
                        answer_relevancy * 0.3)
        
        return EvaluationResult(
            context_relevancy_score=context_relevancy,
            faithfulness_score=faithfulness,
            answer_relevancy_score=answer_relevancy,
            overall_score=overall_score,
            details={
                'query': query,
                'num_contexts': len(retrieved_contexts),
                'answer_length': len(generated_answer),
                'evaluation_timestamp': '2024-01-01T00:00:00Z'  # ì‹¤ì œë¡œëŠ” datetime.now().isoformat()
            }
        )
    
    def batch_evaluate(self, test_cases: List[Dict[str, Any]]) -> List[EvaluationResult]:
        """
        ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•œ ë°°ì¹˜ í‰ê°€
        
        Args:
            test_cases: [{'query': str, 'contexts': List[str], 'answer': str}, ...]
            
        Returns:
            List[EvaluationResult]: ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ í‰ê°€ ê²°ê³¼
        """
        results = []
        for test_case in test_cases:
            result = self.evaluate_rag_system(
                query=test_case['query'],
                retrieved_contexts=test_case['contexts'],
                generated_answer=test_case['answer']
            )
            results.append(result)
        
        return results
    
    def generate_evaluation_report(self, results: List[EvaluationResult]) -> Dict[str, Any]:
        """
        í‰ê°€ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            results: í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸
        """
        if not results:
            return {'error': 'í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'}
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_context_relevancy = np.mean([r.context_relevancy_score for r in results])
        avg_faithfulness = np.mean([r.faithfulness_score for r in results])
        avg_answer_relevancy = np.mean([r.answer_relevancy_score for r in results])
        avg_overall = np.mean([r.overall_score for r in results])
        
        return {
            'summary': {
                'total_evaluations': len(results),
                'average_context_relevancy': float(avg_context_relevancy),
                'average_faithfulness': float(avg_faithfulness),
                'average_answer_relevancy': float(avg_answer_relevancy),
                'average_overall_score': float(avg_overall)
            },
            'performance_level': self._get_performance_level(float(avg_overall)),
            'recommendations': self._generate_recommendations(results)
        }
    
    def _get_performance_level(self, overall_score: float) -> str:
        """ì „ì²´ ì ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë ˆë²¨ ë°˜í™˜"""
        if overall_score >= 0.8:
            return "Excellent"
        elif overall_score >= 0.6:
            return "Good"
        elif overall_score >= 0.4:
            return "Fair"
        else:
            return "Poor"
    
    def _generate_recommendations(self, results: List[EvaluationResult]) -> List[str]:
        """í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        avg_context = np.mean([r.context_relevancy_score for r in results])
        avg_faithfulness = np.mean([r.faithfulness_score for r in results])
        avg_answer = np.mean([r.answer_relevancy_score for r in results])
        
        if avg_context < 0.6:
            recommendations.append("ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ì˜ ê´€ë ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì„ë² ë”© ëª¨ë¸ì´ë‚˜ ê²€ìƒ‰ ì „ëµì„ ê°œì„ í•˜ì„¸ìš”.")
        
        if avg_faithfulness < 0.6:
            recommendations.append("ë‹µë³€ ìƒì„± ì‹œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë” ì¶©ì‹¤íˆ ë°˜ì˜í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ì„¸ìš”.")
        
        if avg_answer < 0.6:
            recommendations.append("ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì˜ ê´€ë ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë‹µë³€ ìƒì„± ë¡œì§ì„ ê°œì„ í•˜ì„¸ìš”.")
        
        if not recommendations:
            recommendations.append("ì „ì²´ì ì¸ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        return recommendations


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
def create_sample_test_cases() -> List[Dict[str, Any]]:
    """ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
    return [
        {
            'query': 'ë‹ˆì•„ì‹ ì•„ë§ˆì´ë“œëŠ” ì–´ë–¤ íš¨ê³¼ê°€ ìˆë‚˜ìš”?',
            'contexts': [
                'ë‹ˆì•„ì‹ ì•„ë§ˆì´ë“œëŠ” ë¹„íƒ€ë¯¼ B3ì˜ í•œ í˜•íƒœë¡œ, í”¼ë¶€ ì§„ì •ê³¼ ìˆ˜ë¶„ ê³µê¸‰ì— íš¨ê³¼ì ì…ë‹ˆë‹¤.',
                'ë‹ˆì•„ì‹ ì•„ë§ˆì´ë“œëŠ” ëª¨ê³µ ì¶•ì†Œì™€ ìƒ‰ì†Œì¹¨ì°© ê°œì„ ì— ë„ì›€ì„ ì¤ë‹ˆë‹¤.'
            ],
            'answer': 'ë‹ˆì•„ì‹ ì•„ë§ˆì´ë“œëŠ” ë¹„íƒ€ë¯¼ B3ì˜ í•œ í˜•íƒœë¡œ, í”¼ë¶€ ì§„ì •, ìˆ˜ë¶„ ê³µê¸‰, ëª¨ê³µ ì¶•ì†Œ, ìƒ‰ì†Œì¹¨ì°© ê°œì„  ë“±ì˜ íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤.'
        },
        {
            'query': 'íˆì•Œë£¨ë¡ ì‚°ì€ ë¬´ì—‡ì¸ê°€ìš”?',
            'contexts': [
                'íˆì•Œë£¨ë¡ ì‚°ì€ ì²œì—° ë³´ìŠµ ì„±ë¶„ìœ¼ë¡œ í”¼ë¶€ì— ìˆ˜ë¶„ì„ ê³µê¸‰í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.',
                'íˆì•Œë£¨ë¡ ì‚°ì€ í”¼ë¶€ íƒ„ë ¥ ê°œì„ ê³¼ ì£¼ë¦„ ì™„í™”ì— íš¨ê³¼ì ì…ë‹ˆë‹¤.'
            ],
            'answer': 'íˆì•Œë£¨ë¡ ì‚°ì€ ì²œì—° ë³´ìŠµ ì„±ë¶„ìœ¼ë¡œ í”¼ë¶€ ìˆ˜ë¶„ ê³µê¸‰ê³¼ íƒ„ë ¥ ê°œì„ ì— íš¨ê³¼ì ì…ë‹ˆë‹¤.'
        }
    ]


def demo_evaluation():
    """í‰ê°€ ì‹œìŠ¤í…œ ë°ëª¨ ì‹¤í–‰"""
    print("ğŸ” RAG ì‹œìŠ¤í…œ í‰ê°€ ë°ëª¨ ì‹œì‘")
    
    evaluator = RAGEvaluator()
    test_cases = create_sample_test_cases()
    
    # ê°œë³„ í‰ê°€ ì‹¤í–‰
    results = evaluator.batch_evaluate(test_cases)
    
    # ê²°ê³¼ ì¶œë ¥
    for i, result in enumerate(results, 1):
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i} í‰ê°€ ê²°ê³¼:")
        print(f"  Context Relevancy: {result.context_relevancy_score:.3f}")
        print(f"  Faithfulness: {result.faithfulness_score:.3f}")
        print(f"  Answer Relevancy: {result.answer_relevancy_score:.3f}")
        print(f"  Overall Score: {result.overall_score:.3f}")
    
    # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    report = evaluator.generate_evaluation_report(results)
    print(f"\nğŸ“ˆ ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸:")
    print(f"  ì´ í‰ê°€ ìˆ˜: {report['summary']['total_evaluations']}")
    print(f"  í‰ê·  Context Relevancy: {report['summary']['average_context_relevancy']:.3f}")
    print(f"  í‰ê·  Faithfulness: {report['summary']['average_faithfulness']:.3f}")
    print(f"  í‰ê·  Answer Relevancy: {report['summary']['average_answer_relevancy']:.3f}")
    print(f"  í‰ê·  Overall Score: {report['summary']['average_overall_score']:.3f}")
    print(f"  ì„±ëŠ¥ ë ˆë²¨: {report['performance_level']}")
    print(f"  ê°œì„  ê¶Œê³ ì‚¬í•­: {', '.join(report['recommendations'])}")


if __name__ == "__main__":
    demo_evaluation()
