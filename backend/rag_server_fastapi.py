#!/usr/bin/env python3
"""
í™”ì¥í’ˆ ì„±ë¶„ RAG ì±—ë´‡ ì„œë²„ - FastAPI ë²„ì „
LangChain, ChromaDB, Chat History, Few-shot í”„ë¡¬í”„íŒ…ì„ í†µí•©í•œ ê³ ê¸‰ êµ¬í˜„
ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
"""

import json
import os
import uuid
from typing import List, Dict, Optional, Any
from datetime import datetime

# FastAPI ê´€ë ¨ imports
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# LangChain ê´€ë ¨ imports
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

import chromadb
from sentence_transformers import SentenceTransformer


# ============================================================================
# Pydantic ëª¨ë¸ ì •ì˜ (íƒ€ì… ì•ˆì „ì„± ë° ìë™ ê²€ì¦)
# ============================================================================

class SearchRequest(BaseModel):
    """ì„±ë¶„ ê²€ìƒ‰ ìš”ì²­"""
    query: str = Field(..., description="ê²€ìƒ‰í•  ì„±ë¶„ëª… ë˜ëŠ” ì§ˆë¬¸")
    session_id: Optional[str] = Field(None, description="ì±„íŒ… ì„¸ì…˜ ID (ì„ íƒ)")


class AnalyzeProductRequest(BaseModel):
    """ì œí’ˆ ë¶„ì„ ìš”ì²­"""
    ingredients: List[str] = Field(..., description="ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸")
    skin_type: str = Field(..., description="ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…")


class GoodMatch(BaseModel):
    """ì¢‹ì€ ì„±ë¶„ ë§¤ì¹­ ê²°ê³¼"""
    name: str
    purpose: str


class BadMatch(BaseModel):
    """ì£¼ì˜ ì„±ë¶„ ë§¤ì¹­ ê²°ê³¼"""
    name: str
    description: str


class AnalyzeProductResponse(BaseModel):
    """ì œí’ˆ ë¶„ì„ ì‘ë‹µ"""
    analysis_report: str
    good_matches: List[GoodMatch]
    bad_matches: List[BadMatch]
    success: bool


class SearchResponse(BaseModel):
    """ê²€ìƒ‰ ì‘ë‹µ"""
    query: str
    answer: str
    similar_ingredients: List[Dict[str, Any]]
    session_id: str
    chat_history: List[Dict[str, str]]
    success: bool


class ChatHistoryResponse(BaseModel):
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì‘ë‹µ"""
    session_id: str
    chat_history: List[Dict[str, str]]
    success: bool


class ClearHistoryRequest(BaseModel):
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ìš”ì²­"""
    session_id: str


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ì²´í¬ ì‘ë‹µ"""
    status: str
    message: str
    ingredients_count: int
    features: List[str]


# ============================================================================
# ë©”ëª¨ë¦¬ ë° LLM í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# ============================================================================

class SimpleConversationMemory:
    """ê°„ë‹¨í•œ ëŒ€í™” ë©”ëª¨ë¦¬ êµ¬í˜„"""
    
    def __init__(self):
        self.messages = []
    
    def save_context(self, inputs: Dict, outputs: Dict):
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì €ì¥"""
        self.messages.append({
            'input': inputs.get('input', ''),
            'output': outputs.get('output', ''),
            'timestamp': datetime.now().isoformat()
        })
    
    def clear(self):
        """ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        self.messages.clear()
    
    @property
    def chat_memory(self):
        """ì±„íŒ… ë©”ëª¨ë¦¬ ê°ì²´ ë°˜í™˜"""
        return self


class MockLLM(LLM):
    """Mock LLM for demonstration purposes - ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    
    @property
    def _llm_type(self) -> str:
        return "mock"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Mock LLM call - í”„ë¡¬í”„íŠ¸ì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±"""
        import re
        
        # ì¢…í•© ë¶„ì„ í”„ë¡¬í”„íŠ¸ ê°ì§€ (ì œí’ˆ ë¶„ì„ ìš”ì²­)
        if "ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸" in prompt or "ì¢…í•© ë¶„ì„" in prompt or "ì´ ì œí’ˆì€ ë‹¤ìŒ ì„±ë¶„ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤" in prompt:
            return self._generate_product_analysis(prompt)
        
        # í”„ë¡¬í”„íŠ¸ì—ì„œ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        lines = prompt.split('\n')
        
        # ì§ˆë¬¸ ì¶”ì¶œ
        question = ""
        context_start = False
        context_parts = []
        in_context_section = False
        
        for line in lines:
            line_stripped = line.strip()
            line_lower = line_stripped.lower()
            
            # ì»¨í…ìŠ¤íŠ¸ ì„¹ì…˜ ì‹œì‘ í™•ì¸
            if "ì»¨í…ìŠ¤íŠ¸" in line_stripped or "context" in line_lower or "==========" in line_stripped:
                if "ì»¨í…ìŠ¤íŠ¸" in line_stripped or "context" in line_lower:
                    in_context_section = True
                continue
            
            # ì§ˆë¬¸ ì„¹ì…˜ í™•ì¸ (ë‹µë³€ ì‹œì‘ ì „ê¹Œì§€)
            if "ì§ˆë¬¸:" in line_stripped or "question:" in line_lower:
                question = line_stripped.split(":", 1)[-1].strip() if ":" in line_stripped else line_stripped
                in_context_section = False
                continue
            
            if "ë‹µë³€:" in line_stripped or "answer:" in line_lower or "========" in line_stripped:
                in_context_section = False
                continue
            
            # ì»¨í…ìŠ¤íŠ¸ ì„¹ì…˜ ë‚´ì˜ ë‚´ìš© ìˆ˜ì§‘
            if in_context_section and line_stripped and not line_stripped.startswith("=="):
                context_parts.append(line_stripped)
            elif not in_context_section and any(keyword in line_stripped for keyword in ["í•œêµ­ì–´ ì„±ë¶„ëª…:", "ì˜ì–´ ì„±ë¶„ëª…:", "ì„¤ëª…:", "ëª©ì :", "ê¶Œì¥ í”¼ë¶€ íƒ€ì…:", "ì£¼ì˜ í”¼ë¶€ íƒ€ì…:", "Korean", "English", "Description"]):
                context_parts.append(line_stripped)
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        if context_parts:
            context_text = "\n".join(context_parts)
            
            # ì§ˆë¬¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            question_lower = question.lower() if question else prompt.lower()
            
            # ì„±ë¶„ëª… ì¶”ì¶œ ì‹œë„ (í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´)
            ingredient_name = ""
            description = ""
            purpose = ""
            good_for = ""
            
            # ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
            full_context = "\n".join(context_parts)
            
            # ì •ê·œí‘œí˜„ì‹ì´ë‚˜ ê°„ë‹¨í•œ íŒŒì‹±ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
            
            # í•œêµ­ì–´ ì„±ë¶„ëª… ì¶”ì¶œ
            kor_match = re.search(r'í•œêµ­ì–´ ì„±ë¶„ëª…:\s*([^\n]+)', full_context, re.IGNORECASE)
            if kor_match:
                ingredient_name = kor_match.group(1).strip()
            
            # ì˜ì–´ ì„±ë¶„ëª… ì¶”ì¶œ (í•œêµ­ì–´ê°€ ì—†ìœ¼ë©´ ì˜ì–´ ì‚¬ìš©)
            eng_match = re.search(r'ì˜ì–´ ì„±ë¶„ëª…:\s*([^\n]+)', full_context, re.IGNORECASE)
            if eng_match:
                eng_name = eng_match.group(1).strip()
                if not ingredient_name:
                    ingredient_name = eng_name
            
            # ì„¤ëª… ì¶”ì¶œ (ë” ë„“ì€ ë²”ìœ„ë¡œ)
            desc_match = re.search(r'ì„¤ëª…:\s*([^\n]+(?:\n(?!í•œêµ­ì–´|ì˜ì–´|ëª©ì |ê¶Œì¥|ì£¼ì˜)[^\n]+)*)', full_context, re.IGNORECASE | re.MULTILINE)
            if desc_match:
                description = desc_match.group(1).strip()[:500]
            
            # ëª©ì  ì¶”ì¶œ
            purpose_match = re.search(r'ëª©ì :\s*([^\n]+)', full_context, re.IGNORECASE)
            if purpose_match:
                purpose = purpose_match.group(1).strip()
            
            # ê¶Œì¥ í”¼ë¶€ íƒ€ì… ì¶”ì¶œ
            good_match = re.search(r'ê¶Œì¥ í”¼ë¶€ íƒ€ì…:\s*([^\n]+)', full_context, re.IGNORECASE)
            if good_match:
                good_for = good_match.group(1).strip()
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
            if ingredient_name and description:
                # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ë‹µë³€ í˜•ì‹ ë³€ê²½
                if any(word in question_lower for word in ["what is", "ë¬´ì—‡", "ë­ì•¼", "ì†Œê°œ", "ì´ìœ "]):
                    answer = f"{ingredient_name}ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ë©´, {description[:300]}"
                elif any(word in question_lower for word in ["íš¨ê³¼", "effect", "ë„ì›€", "help"]):
                    # ëª©ì  ì •ë³´ ì¶”ì¶œ
                    purpose = ""
                    for part in context_parts:
                        if "ëª©ì :" in part:
                            purpose = part.split(":", 1)[-1].strip()
                    if purpose:
                        answer = f"{ingredient_name}ì€(ëŠ”) {purpose} ë“±ì˜ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ë©°, {description[:200]}"
                    else:
                        answer = f"{ingredient_name}ì˜ íš¨ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {description[:300]}"
                else:
                    answer = f"{ingredient_name}ì— ëŒ€í•œ ì •ë³´: {description[:300]}"
                
                # ë‹µë³€ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì„¤ëª…ì„ ë” ì¶”ê°€
                if len(answer) < 100 and description:
                    answer += f"\n\në” ìì„¸íˆ ë§ì”€ë“œë¦¬ë©´, {description[300:600]}"
                
                return answer
            elif description:
                return f"ê²€ìƒ‰ëœ ì •ë³´ì— ë”°ë¥´ë©´: {description[:400]}"
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë‹µë³€
        if "ë‹ˆì•„ì‹ ì•„ë§ˆì´ë“œ" in prompt or "niacinamide" in prompt.lower():
            return "ë‹ˆì•„ì‹ ì•„ë§ˆì´ë“œëŠ” ë¹„íƒ€ë¯¼ B3ì˜ í•œ í˜•íƒœë¡œ, í”¼ë¶€ ì§„ì •ê³¼ ìˆ˜ë¶„ ê³µê¸‰ì— íš¨ê³¼ì ì´ë©° ëª¨ê³µ ì¶•ì†Œì™€ ìƒ‰ì†Œì¹¨ì°© ê°œì„ ì—ë„ ë„ì›€ì„ ì¤ë‹ˆë‹¤."
        elif "íˆì•Œë£¨ë¡ ì‚°" in prompt or "hyaluronic" in prompt.lower():
            return "íˆì•Œë£¨ë¡ ì‚°ì€ ì²œì—° ë³´ìŠµ ì„±ë¶„ìœ¼ë¡œ í”¼ë¶€ì— ìˆ˜ë¶„ì„ ê³µê¸‰í•˜ê³  íƒ„ë ¥ì„ ê°œì„ í•˜ë©° ì£¼ë¦„ ì™„í™”ì— íš¨ê³¼ì ì…ë‹ˆë‹¤."
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì„±ë¶„ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì‹œê±°ë‚˜ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    
    def _generate_product_analysis(self, prompt: str) -> str:
        """ì œí’ˆ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (ìì—°ìŠ¤ëŸ¬ìš´ ì„œìˆ í˜•)"""
        import re
        
        # í”¼ë¶€ íƒ€ì… ì¶”ì¶œ
        skin_type_match = re.search(r'ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…:\s*([^\n]+)', prompt)
        skin_type = skin_type_match.group(1).strip() if skin_type_match else "ì•Œ ìˆ˜ ì—†ëŠ”"
        
        # ì¢‹ì€ ì„±ë¶„ ëª©ë¡ ì¶”ì¶œ (ì´ë¦„ë§Œ)
        good_match_str = re.search(r'\[.*?\]ì— ì¢‹ì€ ì„±ë¶„ ëª©ë¡:\s*([^\n]+)', prompt)
        if not good_match_str:
            good_match_str = re.search(r'ì¢‹ì€ ì„±ë¶„ ëª©ë¡:\s*([^\n]+)', prompt)
        good_names = good_match_str.group(1).strip() if good_match_str else ""
        if good_names == "ì—†ìŒ":
            good_names = ""
        
        # ì£¼ì˜ ì„±ë¶„ ëª©ë¡ ì¶”ì¶œ (ì´ë¦„ë§Œ) - ì¼ë°˜ì  í¬í•¨ í˜•ì‹ë„ ì§€ì›
        bad_match_str = re.search(r'ì£¼ì˜ ì„±ë¶„ ëª©ë¡ \(ì¼ë°˜ì  í¬í•¨\):\s*([^\n]+)', prompt)
        if not bad_match_str:
            bad_match_str = re.search(r'\[.*?\]ì— ì£¼ì˜ ì„±ë¶„ ëª©ë¡:\s*([^\n]+)', prompt)
        if not bad_match_str:
            bad_match_str = re.search(r'ì£¼ì˜ ì„±ë¶„ ëª©ë¡:\s*([^\n]+)', prompt)
        bad_names = bad_match_str.group(1).strip() if bad_match_str else ""
        if bad_names == "ì—†ìŒ":
            bad_names = ""
        
        # ì¢‹ì€ ì„±ë¶„ ì„¸ë¶€ì •ë³´ ì¶”ì¶œ
        good_details_match = re.search(r'ì¢‹ì€ ì„±ë¶„ ì„¸ë¶€ì •ë³´:\s*([^\n]+(?:\n- [^\n]+)*)', prompt, re.MULTILINE)
        good_details = good_details_match.group(1).strip() if good_details_match else ""
        if good_details == "ì—†ìŒ":
            good_details = ""
        
        # ì£¼ì˜ ì„±ë¶„ ì„¸ë¶€ì •ë³´ ì¶”ì¶œ
        bad_details_match = re.search(r'ì£¼ì˜ ì„±ë¶„ ì„¸ë¶€ì •ë³´:\s*([^\n]+(?:\n- [^\n]+)*)', prompt, re.MULTILINE)
        bad_details = bad_details_match.group(1).strip() if bad_details_match else ""
        if bad_details == "ì—†ìŒ":
            bad_details = ""
        
        # ëª©ì  ì¶”ë¡  (ê°€ì¥ ë¹ˆë²ˆí•œ ëª©ì  1ê°œ)
        purpose_match = re.search(r'ì£¼ìš” ì„±ë¶„ ëª©ì \):\s*([^\n]+)', prompt)
        main_purpose = "ë³µí•©ì ì¸"  # ê¸°ë³¸ê°’
        
        if purpose_match:
            purposes = purpose_match.group(1).strip()
            if purposes:
                # "moisturizer (3íšŒ)" ê°™ì€ í˜•ì‹ì—ì„œ "moisturizer"ë§Œ ì¶”ì¶œ
                first_purpose_match = re.search(r'([a-zA-Zê°€-í£\s]+)\s*\(\d+íšŒ\)', purposes)
                if first_purpose_match:
                    purpose_name = first_purpose_match.group(1).strip()
                    
                    # purpose í•œê¸€í™” ë§¤í•‘
                    purpose_map = {
                        "anti-acne": "ì—¬ë“œë¦„ ì™„í™”",
                        "antioxidant": "í•­ì‚°í™”",
                        "cleansing": "ì„¸ì •",
                        "colorant": "ì°©ìƒ‰",
                        "emulsifier": "ìœ í™”",
                        "exfoliant": "ê°ì§ˆ ì œê±°",
                        "fragrance": "í–¥ë£Œ",
                        "moisturizer": "ë³´ìŠµ",
                        "ph adjuster": "pH ì¡°ì ˆ",
                        "preservative": "ë³´ì¡´",
                        "skin-brightening": "ë¯¸ë°±",
                        "solvent": "ìš©í•´",
                        "soothing": "ì§„ì •",
                        "sunscreen": "ìì™¸ì„  ì°¨ë‹¨",
                        "thickener": "ì ì¦",
                        # ë™ì˜ì–´/ë³€í˜•ì–´
                        "emollient": "ë³´ìŠµ ë° ìœ ì—°í™”",
                        "humectant": "ìˆ˜ë¶„ ê³µê¸‰",
                        "anti-aging": "í•­ë…¸í™”",
                        "whitening": "ë¯¸ë°±",
                        "anti-inflammatory": "í•­ì—¼",
                        "antimicrobial": "í•­ê· ",
                        "moisturizing": "ë³´ìŠµ",
                        "conditioning": "ì»¨ë””ì…”ë‹"
                    }
                    
                    # ì˜ì–´ ë˜ëŠ” í•œê¸€ ë§¤ì¹­
                    purpose_lower = purpose_name.lower()
                    main_purpose = purpose_map.get(purpose_lower, purpose_name)
                    
                    # í•œê¸€ì´ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    if any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in purpose_name):
                        main_purpose = purpose_name
        
        # --- ì„œìˆ í˜• ë¦¬í¬íŠ¸ ìƒì„± ---
        report_parts = []
        
        # 1. ì œí’ˆ íƒ€ì…/ëª©ì  ì¶”ë¡ 
        product_type = None
        if main_purpose == "í–¥ë£Œ" or "fragrance" in bad_details.lower() or "fragrance" in good_details.lower():
            product_type = "í–¥ìˆ˜"
            report_parts.append(f"ì´ í™”ì¥í’ˆì€(ëŠ”) í–¥ìˆ˜ ì œí’ˆìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.")
        else:
            report_parts.append(f"ì´ í™”ì¥í’ˆì€(ëŠ”) '{main_purpose}'ì— ì¤‘ì ì„ ë‘” ì œí’ˆìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.")
        
        # 2. ê¸ì •ì  ë¶„ì„
        if good_names:
            # ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸ ì •ë¦¬ (ì²« 2-3ê°œë§Œ ì–¸ê¸‰)
            good_names_list = [name.strip() for name in good_names.split(',') if name.strip()][:3]
            good_names_short = ", ".join(good_names_list)
            if len([name.strip() for name in good_names.split(',') if name.strip()]) > 3:
                good_names_short += " ë“±"
            
            # ì¢‹ì€ ì„±ë¶„ì˜ ì£¼ìš” ëª©ì  ì¶”ì¶œ
            main_benefit = "í”¼ë¶€ ê°œì„ "
            if good_details:
                # purposeì—ì„œ ì£¼ìš” íš¨ëŠ¥ ì¶”ì¶œ ì‹œë„
                if "ë³´ìŠµ" in good_details or "moisturizer" in good_details.lower():
                    main_benefit = "ë³´ìŠµ ë° ì»¨ë””ì…”ë‹"
                elif "ì§„ì •" in good_details or "soothing" in good_details.lower():
                    main_benefit = "ì§„ì • ë° ì¼€ì–´"
                elif "í•­ì‚°í™”" in good_details or "antioxidant" in good_details.lower():
                    main_benefit = "í•­ì‚°í™” ë° ë³´í˜¸"
            
            # í”¼ë¶€ íƒ€ì…ë³„ ì •ë³´ê°€ ìˆìœ¼ë©´ ì–¸ê¸‰
            if skin_type != "ì•Œ ìˆ˜ ì—†ëŠ”":
                report_parts.append(f"íŠ¹íˆ {skin_type} í”¼ë¶€ì— ì¢‹ì€ {good_names_short} ì„±ë¶„ì´ í¬í•¨ë˜ì–´ ìˆì–´, {main_benefit}ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                report_parts.append(f"ì´ ì œí’ˆì—ëŠ” {good_names_short} ì„±ë¶„ì´ í¬í•¨ë˜ì–´ ìˆì–´, {main_benefit}ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            if product_type == "í–¥ìˆ˜":
                report_parts.append(f"ì´ ì œí’ˆì€ í–¥ë£Œ ì„±ë¶„ì— ì¤‘ì ì„ ë‘” í–¥ìˆ˜ ì œí’ˆì…ë‹ˆë‹¤.")
            else:
                report_parts.append(f"íŠ¹ë³„íˆ {skin_type} í”¼ë¶€ì— ìœ ìµí•œ ì„±ë¶„ì€ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # 3. ë¶€ì •ì  ë¶„ì„ (ì¼ë°˜ì ì¸ ì£¼ì˜ì‚¬í•­ í¬í•¨)
        if bad_names:
            # ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸ ì •ë¦¬ (ì²« 1-2ê°œë§Œ ì–¸ê¸‰)
            bad_names_list = [name.strip() for name in bad_names.split(',') if name.strip()][:2]
            bad_names_short = ", ".join(bad_names_list)
            if len([name.strip() for name in bad_names.split(',') if name.strip()]) > 2:
                bad_names_short += " ë“±"
            
            # ì£¼ì˜ ì´ìœ  ì¶”ì¶œ (bad_detailsì—ì„œ ë¯¼ê°ì„± ë“± ì¶”ì¶œ)
            caution_reason = "ìê·¹"
            caution_target = None
            
            if bad_details:
                bad_details_lower = bad_details.lower()
                # í•œê¸€ í‚¤ì›Œë“œ í™•ì¸
                if "ë¯¼ê°ì„±" in bad_details or "sensitive" in bad_details_lower:
                    caution_reason = "ì•Œë ˆë¥´ê¸° ë°˜ì‘"
                    caution_target = "ë¯¼ê°ì„± í”¼ë¶€ë¥¼ ê°€ì§„ ë¶„ë“¤"
                elif "ì—¬ë“œë¦„" in bad_details or "acne" in bad_details_lower:
                    caution_reason = "ì—¬ë“œë¦„ ì•…í™”"
                    caution_target = "ì—¬ë“œë¦„ì„± í”¼ë¶€ë¥¼ ê°€ì§„ ë¶„ë“¤"
                elif "ì—¬ë“œë¦„ì„±" in bad_details or "acne-prone" in bad_details_lower:
                    caution_reason = "ì—¬ë“œë¦„ ì•…í™”"
                    caution_target = "ì—¬ë“œë¦„ì„± í”¼ë¶€ë¥¼ ê°€ì§„ ë¶„ë“¤"
                elif "ì•Œë ˆë¥´ê¸°" in bad_details or "allergy" in bad_details_lower:
                    caution_reason = "ì•Œë ˆë¥´ê¸° ë°˜ì‘"
                    caution_target = "ì•Œë ˆë¥´ê¸° ì²´ì§ˆì¸ ë¶„ë“¤"
                elif "ê±´ì¡°" in bad_details or "dry" in bad_details_lower:
                    caution_reason = "ê±´ì¡° ìœ ë°œ"
                    caution_target = f"{skin_type} í”¼ë¶€"
                elif "ìê·¹" in bad_details or "irritation" in bad_details_lower:
                    caution_reason = "í”¼ë¶€ ìê·¹"
                    caution_target = "ì¼ë¶€ ì‚¬ìš©ì"
            
            # ì œí’ˆ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í‘œí˜„
            if product_type == "í–¥ìˆ˜":
                if caution_target:
                    report_parts.append(f"ë‹¤ë§Œ, {bad_names_short} ë“±ì˜ í–¥ë£Œ ì„±ë¶„ì€ {caution_target}ì—ê²Œ {caution_reason}ì„ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë‹ˆ, í–¥ë£Œ ì•Œë ˆë¥´ê¸°ë‚˜ ë¯¼ê°ì„± í”¼ë¶€ê°€ ìˆìœ¼ì‹  ë¶„ë“¤ì€ ì‚¬ìš© ì „ íŒ¨ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
                else:
                    report_parts.append(f"ë‹¤ë§Œ, {bad_names_short} ë“±ì˜ í–¥ë£Œ ì„±ë¶„ì€ ì¼ë¶€ ì‚¬ìš©ìì—ê²Œ ì•Œë ˆë¥´ê¸° ë°˜ì‘ì„ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë‹ˆ, í–¥ë£Œ ì•Œë ˆë¥´ê¸°ë‚˜ ë¯¼ê°ì„± í”¼ë¶€ê°€ ìˆìœ¼ì‹  ë¶„ë“¤ì€ ì‚¬ìš© ì „ íŒ¨ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            else:
                if caution_target:
                    report_parts.append(f"ë‹¤ë§Œ, {bad_names_short} ì„±ë¶„ì€ {caution_target}ì—ê²Œ {caution_reason}ì„ ìœ ë°œí•  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
                else:
                    report_parts.append(f"ë‹¤ë§Œ, {bad_names_short} ì„±ë¶„ì€ {skin_type} í”¼ë¶€ì— {caution_reason}ì„ ìœ ë°œí•  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
        else:
            if product_type == "í–¥ìˆ˜":
                report_parts.append(f"ë‹¤ë§Œ, í–¥ìˆ˜ ì œí’ˆì˜ íŠ¹ì„±ìƒ í–¥ë£Œ ì„±ë¶„ì´ í¬í•¨ë˜ì–´ ìˆì–´, í–¥ë£Œ ì•Œë ˆë¥´ê¸°ë‚˜ ë¯¼ê°ì„± í”¼ë¶€ê°€ ìˆìœ¼ì‹  ë¶„ë“¤ì€ ì‚¬ìš© ì „ íŒ¨ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            else:
                report_parts.append(f"ë‹¤í–‰íˆ {skin_type} í”¼ë¶€ì— íŠ¹ë³„íˆ ì£¼ì˜ê°€ í•„ìš”í•œ ì„±ë¶„ì€ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # 4. ìµœì¢… ê¶Œì¥ì‚¬í•­
        if product_type == "í–¥ìˆ˜":
            if bad_names:
                report_parts.append(f"ë”°ë¼ì„œ ì´ í–¥ìˆ˜ ì œí’ˆì€ ì¼ë°˜ì ì¸ ì‚¬ìš©ì—ëŠ” ë¬´ë‚œí•˜ì§€ë§Œ, í–¥ë£Œ ì•Œë ˆë¥´ê¸°ë‚˜ ë¯¼ê°ì„± í”¼ë¶€ë¥¼ ê°€ì§„ ë¶„ë“¤ì€ ì‚¬ìš© ì „ ë°˜ë“œì‹œ íŒ¨ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ì‹œê³ , í”¼ë¶€ ë°˜ì‘ì´ ì¢‹ì§€ ì•Šìœ¼ë©´ ì‚¬ìš©ì„ ì¤‘ë‹¨í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
            else:
                report_parts.append(f"ë”°ë¼ì„œ ì´ í–¥ìˆ˜ ì œí’ˆì€ ì¼ë°˜ì ì¸ ì‚¬ìš©ì—ëŠ” ë¬´ë‚œí•˜ì§€ë§Œ, í–¥ë£Œ ì„±ë¶„ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì‚¬ìš© ì „ ê°œì¸ì ì¸ í”¼ë¶€ ë°˜ì‘ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
        else:
            if good_names and not bad_names:
                report_parts.append(f"ë”°ë¼ì„œ ì´ ì œí’ˆì€ {skin_type} í”¼ë¶€ì— ëŒ€í•´ ì „ë°˜ì ìœ¼ë¡œ ê¸ì •ì ì¸ í‰ê°€ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¢‹ì€ ì„±ë¶„ë“¤ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            elif good_names and bad_names:
                report_parts.append(f"ë”°ë¼ì„œ ì´ ì œí’ˆì€ {skin_type} í”¼ë¶€ì— ë„ì›€ì´ ë˜ëŠ” ì„±ë¶„ê³¼ ì£¼ì˜ê°€ í•„ìš”í•œ ì„±ë¶„ì´ í˜¼ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš© ì‹œ í”¼ë¶€ ë°˜ì‘ì„ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•˜ì‹œê³ , ì²˜ìŒ ì‚¬ìš© ì‹œ ì†ŒëŸ‰ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.")
            elif bad_names and not good_names:
                report_parts.append(f"ë”°ë¼ì„œ ì´ ì œí’ˆì€ {skin_type} í”¼ë¶€ íƒ€ì…ì—ê²Œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ìš© ì „ ë°˜ë“œì‹œ íŒ¨ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ì‹œê³ , í”¼ë¶€ ë°˜ì‘ì´ ì¢‹ì§€ ì•Šìœ¼ë©´ ì‚¬ìš©ì„ ì¤‘ë‹¨í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
            else:
                report_parts.append(f"ë”°ë¼ì„œ ì´ ì œí’ˆì€ {skin_type} í”¼ë¶€ì— ëŒ€í•´ ì¤‘ë¦½ì ì¸ í‰ê°€ì…ë‹ˆë‹¤. ê°œì¸ì ì¸ í”¼ë¶€ ë°˜ì‘ì„ í™•ì¸í•˜ë©° ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
        
        return " ".join(report_parts)


# ============================================================================
# EnterpriseRAG í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼í•˜ë‚˜ ë¹„ë™ê¸° ë©”ì„œë“œ ì¶”ê°€)
# ============================================================================

class EnterpriseRAG:
    """ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ RAG ì‹œìŠ¤í…œ - LangChain, ChromaDB, Chat History í†µí•©"""
    
    def __init__(self, data_file: str, persist_directory: str = "./chroma_db_ingredients"):
        """ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.data_file = data_file
        self.persist_directory = persist_directory
        self.ingredients_data = []
        
        # LangChain ì»´í¬ë„ŒíŠ¸ë“¤
        self.text_splitter = None
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.qa_runnable = None
        self.few_shot_prompt = None
        
        # Chat History ê´€ë¦¬
        self.chat_sessions = {}  # session_id -> SimpleConversationMemory
        
        # ì´ˆê¸°í™” ì‹¤í–‰
        self.load_data()
        self.initialize_components()
        self.create_vectorstore()
        self.create_few_shot_prompt()
        self.create_qa_runnable()
    
    def load_data(self):
        """JSON ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“š í™”ì¥í’ˆ ì„±ë¶„ ë°ì´í„° ë¡œë”© ì¤‘...")
        with open(self.data_file, 'r', encoding='utf-8') as f:
            self.ingredients_data = json.load(f)
        print(f"âœ… {len(self.ingredients_data)}ê°œ ì„±ë¶„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    
    def initialize_components(self):
        """LangChain ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”"""
        print("ğŸ”§ LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (í•œêµ­ì–´ ì§€ì›)
        self.embeddings = SentenceTransformerEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        
        # Mock LLM ì´ˆê¸°í™”
        self.llm = MockLLM()
        
        print("âœ… LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def create_vectorstore(self):
        """ChromaDB ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        print("ğŸ—„ï¸ ChromaDB ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        
        # ë¬¸ì„œ ìƒì„±
        documents = []
        for item in self.ingredients_data:
            kor_name = item.get('INGR_KOR_NAME', '')
            eng_name = item.get('INGR_ENG_NAME', '')
            description = item.get('description', '')
            purpose = item.get('purpose', [])
            good_for = item.get('good_for', [])
            bad_for = item.get('bad_for', [])
            
            # í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìƒì„±
            content_parts = []
            if kor_name:
                content_parts.append(f"í•œêµ­ì–´ ì„±ë¶„ëª…: {kor_name}")
            if eng_name:
                content_parts.append(f"ì˜ì–´ ì„±ë¶„ëª…: {eng_name}")
            if description:
                content_parts.append(f"ì„¤ëª…: {description}")
            if purpose:
                content_parts.append(f"ëª©ì : {', '.join(purpose) if isinstance(purpose, list) else purpose}")
            if good_for:
                content_parts.append(f"ê¶Œì¥ í”¼ë¶€ íƒ€ì…: {', '.join(good_for) if isinstance(good_for, list) else good_for}")
            if bad_for:
                content_parts.append(f"ì£¼ì˜ í”¼ë¶€ íƒ€ì…: {', '.join(bad_for) if isinstance(bad_for, list) else bad_for}")
            
            content = "\n".join(content_parts)
            
            doc = Document(
                page_content=content,
                metadata={
                    "ingredient_kor": kor_name,
                    "ingredient_eng": eng_name,
                    "description": (description[:200] + "..." if description and len(description) > 200 else description) or '',
                    "purpose": ', '.join(purpose) if isinstance(purpose, list) else (purpose or ''),
                    "good_for": ', '.join(good_for) if isinstance(good_for, list) else (good_for or ''),
                    "bad_for": ', '.join(bad_for) if isinstance(bad_for, list) else (bad_for or '')
                }
            )
            documents.append(doc)
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        split_docs = self.text_splitter.split_documents(documents)
        print(f"ğŸ“„ {len(documents)}ê°œ ë¬¸ì„œë¥¼ {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        # ChromaDB ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        self.vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        
        print("âœ… ChromaDB ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
    
    def create_few_shot_prompt(self):
        """Few-shot í”„ë¡¬í”„íŒ… í…œí”Œë¦¿ ìƒì„±"""
        print("ğŸ¯ Few-shot í”„ë¡¬í”„íŒ… í…œí”Œë¦¿ ìƒì„± ì¤‘...")
        
        # Few-shot ì˜ˆì‹œë“¤
        examples = [
            {
                "question": "ë‹ˆì•„ì‹ ì•„ë§ˆì´ë“œëŠ” ì–´ë–¤ íš¨ê³¼ê°€ ìˆë‚˜ìš”?",
                "answer": "ë‹ˆì•„ì‹ ì•„ë§ˆì´ë“œëŠ” ë¹„íƒ€ë¯¼ B3ì˜ í•œ í˜•íƒœë¡œ, í”¼ë¶€ ì§„ì •ê³¼ ìˆ˜ë¶„ ê³µê¸‰ì— íš¨ê³¼ì ì…ë‹ˆë‹¤. ë˜í•œ ëª¨ê³µ ì¶•ì†Œì™€ ìƒ‰ì†Œì¹¨ì°© ê°œì„ ì—ë„ ë„ì›€ì„ ì£¼ëŠ” ì„±ë¶„ì…ë‹ˆë‹¤."
            },
            {
                "question": "íˆì•Œë£¨ë¡ ì‚°ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "answer": "íˆì•Œë£¨ë¡ ì‚°ì€ ì²œì—° ë³´ìŠµ ì„±ë¶„ìœ¼ë¡œ, í”¼ë¶€ì— ìˆ˜ë¶„ì„ ê³µê¸‰í•˜ê³  íƒ„ë ¥ì„ ê°œì„ í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì£¼ë¦„ ì™„í™”ì™€ í”¼ë¶€ ë³´ìŠµì— ë§¤ìš° íš¨ê³¼ì ì¸ ì„±ë¶„ì…ë‹ˆë‹¤."
            },
            {
                "question": "ë ˆí‹°ë†€ì˜ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "answer": "ë ˆí‹°ë†€ì€ ê°•ë ¥í•œ í•­ë…¸í™” ì„±ë¶„ì´ì§€ë§Œ, ìì™¸ì„ ì— ë¯¼ê°í•˜ë¯€ë¡œ ì£¼ê°„ ì‚¬ìš© ì‹œ ë°˜ë“œì‹œ ìì™¸ì„  ì°¨ë‹¨ì œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ ì²˜ìŒ ì‚¬ìš© ì‹œ í”¼ë¶€ ìê·¹ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ì§„ì ìœ¼ë¡œ ì‚¬ìš©ëŸ‰ì„ ëŠ˜ë ¤ê°€ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."
            }
        ]
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        example_prompt = PromptTemplate(
            input_variables=["question", "answer"],
            template="ì§ˆë¬¸: {question}\në‹µë³€: {answer}"
        )
        
        # Few-shot í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        self.few_shot_prompt = FewShotPromptTemplate(
            examples=examples,
            example_prompt=example_prompt,
            prefix="ë‹¹ì‹ ì€ í™”ì¥í’ˆ ì„±ë¶„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n\nì˜ˆì‹œ:",
            suffix="\n\nì§ˆë¬¸: {question}\në‹µë³€:",
            input_variables=["question"]
        )
        
        print("âœ… Few-shot í”„ë¡¬í”„íŒ… í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ")
    
    def create_qa_runnable(self):
        """Runnable ê¸°ë°˜ QA íŒŒì´í”„ë¼ì¸ ìƒì„±"""
        print("ğŸ”— Runnable QA íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
        
        # Retriever ì„¤ì •
        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 3})
        
        # ì»¤ìŠ¤í…€ Runnable ì²´ì¸ ìƒì„±
        def custom_qa_chain(input_dict):
            """ì»¤ìŠ¤í…€ QA ì²´ì¸: ê²€ìƒ‰ -> ë¬¸ì„œ ì§ì ‘ ì‚¬ìš© -> ë‹µë³€ ìƒì„±"""
            question = input_dict["question"]
            # ê²€ìƒ‰ ìˆ˜í–‰
            retriever = self.retriever
            docs = retriever.invoke(question)
            
            # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë‹µë³€
            if not docs:
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì„±ë¶„ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return {"answer": answer, "docs": []}
            
            # ì²« ë²ˆì§¸ ë¬¸ì„œì—ì„œ ì •ë³´ ì¶”ì¶œ
            first_doc = docs[0]
            doc_content = first_doc.page_content
            metadata = first_doc.metadata
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
            ingredient_kor = metadata.get("ingredient_kor", "")
            ingredient_eng = metadata.get("ingredient_eng", "")
            description_meta = metadata.get("description", "")
            purpose = metadata.get("purpose", "")
            
            # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
            ingredient_name = ingredient_kor or ingredient_eng or ""
            
            # ì„¤ëª… ì¶”ì¶œ
            description = description_meta
            if not description or len(description) < 50:
                # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì„¤ëª… ì¶”ì¶œ
                import re
                desc_match = re.search(r'ì„¤ëª…:\s*([^\n]+(?:\n[^\n]+)*)', doc_content, re.IGNORECASE | re.MULTILINE)
                if desc_match:
                    description = desc_match.group(1).strip()[:500]
                elif doc_content:
                    description = doc_content[:500]
            
            # ë‹µë³€ ìƒì„±
            if ingredient_name and description:
                question_lower = question.lower()
                if any(word in question_lower for word in ["what is", "ë¬´ì—‡", "ë­ì•¼", "ì†Œê°œ"]):
                    answer = f"{ingredient_name}ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ë©´, {description[:400]}"
                elif any(word in question_lower for word in ["íš¨ê³¼", "effect", "ë„ì›€", "help"]):
                    if purpose:
                        answer = f"{ingredient_name}ì€(ëŠ”) {purpose} ë“±ì˜ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë˜ë©°, {description[:300]}"
                    else:
                        answer = f"{ingredient_name}ì˜ íš¨ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {description[:400]}"
                else:
                    answer = f"{ingredient_name}ì— ëŒ€í•œ ì •ë³´: {description[:400]}"
            elif description:
                answer = f"ê²€ìƒ‰ëœ ì •ë³´ì— ë”°ë¥´ë©´: {description[:400]}"
            else:
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì„±ë¶„ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            return {"answer": answer, "docs": docs}
        
        self.qa_runnable = lambda input_dict: custom_qa_chain(input_dict)
        
        print("âœ… Runnable QA íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ")
    
    def get_or_create_session(self, session_id: str = None) -> str:
        """ì±„íŒ… ì„¸ì…˜ ìƒì„± ë˜ëŠ” ì¡°íšŒ"""
        if session_id is None:
            session_id = str(uuid.uuid4())
        
        if session_id not in self.chat_sessions:
            self.chat_sessions[session_id] = SimpleConversationMemory()
        
        return session_id
    
    def search_ingredients(self, query: str, session_id: str = None, top_k: int = 3) -> Dict:
        """LangChain ê¸°ë°˜ ì„±ë¶„ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±"""
        # ì„¸ì…˜ ê´€ë¦¬
        session_id = self.get_or_create_session(session_id)
        memory = self.chat_sessions[session_id]
        
        try:
            # Runnable íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            result = self.qa_runnable({"question": query})
            answer = result["answer"]
            docs = result["docs"]
            
            similar_ingredients = []
            for doc in docs:
                ingredient_info = {
                    "ingredient_kor": doc.metadata.get("ingredient_kor", ""),
                    "ingredient_eng": doc.metadata.get("ingredient_eng", ""),
                    "description": doc.metadata.get("description", ""),
                    "purpose": doc.metadata.get("purpose", ""),
                    "good_for": doc.metadata.get("good_for", ""),
                    "bad_for": doc.metadata.get("bad_for", ""),
                    "similarity": 0.8
                }
                similar_ingredients.append(ingredient_info)
            
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            memory.save_context(
                {"input": query},
                {"output": answer}
            )
            
            return {
                "query": query,
                "answer": answer,
                "similar_ingredients": similar_ingredients,
                "session_id": session_id,
                "chat_history": memory.messages[-4:] if len(memory.messages) > 4 else memory.messages,
                "success": True
            }
            
        except Exception as e:
            return {
                "query": query,
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "similar_ingredients": [],
                "session_id": session_id,
                "chat_history": [],
                "success": False
            }
    
    def get_chat_history(self, session_id: str) -> Dict:
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        if session_id not in self.chat_sessions:
            return {"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "chat_history": []}
        
        memory = self.chat_sessions[session_id]
        return {
            "session_id": session_id,
            "chat_history": memory.messages,
            "success": True
        }
    
    def clear_chat_history(self, session_id: str) -> Dict:
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        if session_id in self.chat_sessions:
            self.chat_sessions[session_id].clear()
            return {"message": "ì±„íŒ… íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.", "success": True}
        else:
            return {"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "success": False}
    
    def analyze_product_ingredients(self, ingredients: List[str], skin_type: str) -> Dict:
        """
        ì œí’ˆ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ì™€ í”¼ë¶€ íƒ€ì…ì„ ê¸°ë°˜ìœ¼ë¡œ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        """
        try:
            # ì„±ë¶„ëª… ì •ê·œí™” í•¨ìˆ˜
            def normalize_ingredient_name(name: str) -> str:
                return name.strip().lower().replace(" ", "").replace("-", "").replace("_", "")
            
            # 1. ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            ingredient_info_map = {}
            
            for ingredient_name in ingredients:
                normalized_name = normalize_ingredient_name(ingredient_name)
                
                # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
                matched_ingredient = None
                for item in self.ingredients_data:
                    kor_name = item.get('INGR_KOR_NAME', '')
                    eng_name = item.get('INGR_ENG_NAME', '')
                    
                    if (kor_name and normalize_ingredient_name(kor_name) == normalized_name) or \
                       (eng_name and normalize_ingredient_name(eng_name) == normalized_name):
                        matched_ingredient = item
                        break
                
                # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                if not matched_ingredient and len(normalized_name) >= 3:
                    for item in self.ingredients_data:
                        kor_name = item.get('INGR_KOR_NAME', '')
                        eng_name = item.get('INGR_ENG_NAME', '')
                        
                        kor_normalized = normalize_ingredient_name(kor_name) if kor_name else ""
                        eng_normalized = normalize_ingredient_name(eng_name) if eng_name else ""
                        
                        if (kor_normalized and (normalized_name in kor_normalized or kor_normalized in normalized_name)) or \
                           (eng_normalized and (normalized_name in eng_normalized or eng_normalized in normalized_name)):
                            matched_ingredient = item
                            break
                
                # ë§¤ì¹­ëœ ì„±ë¶„ ì •ë³´ ì €ì¥
                if matched_ingredient:
                    kor_name = matched_ingredient.get('INGR_KOR_NAME', '')
                    eng_name = matched_ingredient.get('INGR_ENG_NAME', '')
                    description = matched_ingredient.get('description', '')
                    purpose = matched_ingredient.get('purpose', [])
                    good_for = matched_ingredient.get('good_for', [])
                    bad_for = matched_ingredient.get('bad_for', [])
                    
                    ingredient_info_map[ingredient_name] = {
                        "name": kor_name or eng_name,
                        "eng_name": eng_name,
                        "description": description,
                        "purpose": ', '.join(purpose) if isinstance(purpose, list) else (purpose or ''),
                        "good_for": ', '.join(good_for) if isinstance(good_for, list) else (good_for or ''),
                        "bad_for": ', '.join(bad_for) if isinstance(bad_for, list) else (bad_for or ''),
                        "page_content": f"í•œêµ­ì–´ ì„±ë¶„ëª…: {kor_name}\nì˜ì–´ ì„±ë¶„ëª…: {eng_name}\nì„¤ëª…: {description}"
                    }
                else:
                    # ë²¡í„° ê²€ìƒ‰
                    docs = self.retriever.invoke(ingredient_name)
                    if docs:
                        best_doc = docs[0]
                        metadata = best_doc.metadata
                        ingredient_info_map[ingredient_name] = {
                            "name": metadata.get("ingredient_kor", "") or metadata.get("ingredient_eng", ""),
                            "eng_name": metadata.get("ingredient_eng", ""),
                            "description": metadata.get("description", ""),
                            "purpose": metadata.get("purpose", ""),
                            "good_for": metadata.get("good_for", ""),
                            "bad_for": metadata.get("bad_for", ""),
                            "page_content": best_doc.page_content
                        }
            
            # 2. ë°ì´í„° ì§‘ê³„
            good_matches = []
            bad_matches = []
            good_ingredient_names = []
            bad_ingredient_names = []
            
            # í”¼ë¶€ íƒ€ì… ë§¤í•‘
            skin_type_map = {
                "acne": "ì—¬ë“œë¦„",
                "acne-prone": "ì—¬ë“œë¦„ì„±",
                "damaged": "ì†ìƒëœ",
                "dry": "ê±´ì„±",
                "irritated": "ìê·¹ë°›ì€",
                "oily": "ì§€ì„±",
                "sensitive": "ë¯¼ê°ì„±"
            }
            
            good_for_map = {
                "acne": "ì—¬ë“œë¦„",
                "acne-prone": "ì—¬ë“œë¦„ì„±",
                "damaged": "ì†ìƒëœ",
                "dry": "ê±´ì„±",
                "irritated": "ìê·¹ë°›ì€",
                "oily": "ì§€ì„±",
                "sensitive": "ë¯¼ê°ì„±"
            }
            
            bad_for_map = {
                "acne": "ì—¬ë“œë¦„",
                "acne-prone": "ì—¬ë“œë¦„ì„±",
                "sensitive": "ë¯¼ê°ì„±"
            }
            
            normalized_skin_type = skin_type_map.get(skin_type.lower(), skin_type)
            
            general_caution_keywords = ["sensitive", "allergy", "irritation", "acne", "acne-prone"]
            general_caution_keywords_kr = ["ë¯¼ê°ì„±", "ì•Œë ˆë¥´ê¸°", "ìê·¹", "ì—¬ë“œë¦„", "ì—¬ë“œë¦„ì„±"]
            
            for ingredient_name, info in ingredient_info_map.items():
                good_for = info.get("good_for", "")
                bad_for = info.get("bad_for", "")
                purpose = info.get("purpose", "")
                description = info.get("description", "")
                
                # good_for ë¶„ì„
                if good_for:
                    good_for_list = [x.strip() for x in good_for.split(',') if x.strip()]
                    good_for_list_normalized = [
                        good_for_map.get(item.lower(), item) 
                        for item in good_for_list
                    ]
                    if normalized_skin_type in good_for_list or normalized_skin_type in good_for_list_normalized:
                        good_matches.append({
                            "name": info.get("name", ingredient_name),
                            "purpose": purpose if purpose else "ê¸°ëŠ¥ ì •ë³´ ì—†ìŒ"
                        })
                        good_ingredient_names.append(info.get("name", ingredient_name))
                
                # bad_for ë¶„ì„
                if bad_for:
                    bad_for_list = [x.strip() for x in bad_for.split(',') if x.strip()]
                    bad_for_list_normalized = [
                        bad_for_map.get(item.lower(), item) 
                        for item in bad_for_list
                    ]
                    
                    # í•´ë‹¹ í”¼ë¶€ íƒ€ì…ì— ì£¼ì˜ê°€ í•„ìš”í•œ ê²½ìš°
                    if normalized_skin_type in bad_for_list or normalized_skin_type in bad_for_list_normalized:
                        short_desc = description[:100] + "..." if len(description) > 100 else description
                        bad_matches.append({
                            "name": info.get("name", ingredient_name),
                            "description": short_desc if short_desc else f"{skin_type} í”¼ë¶€ì— ì£¼ì˜ê°€ í•„ìš”í•œ ì„±ë¶„ì…ë‹ˆë‹¤."
                        })
                        bad_ingredient_names.append(info.get("name", ingredient_name))
                    # ì¼ë°˜ì ì¸ ì£¼ì˜ì‚¬í•­
                    elif any(keyword in bad_for_list for keyword in general_caution_keywords) or \
                         any(keyword in bad_for_list for keyword in general_caution_keywords_kr):
                        if info.get("name", ingredient_name) not in bad_ingredient_names:
                            caution_reason = None
                            bad_for_lower = [x.lower() for x in bad_for_list]
                            
                            if "sensitive" in bad_for_lower or "ë¯¼ê°ì„±" in bad_for_list:
                                caution_reason = "ë¯¼ê°ì„± í”¼ë¶€ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤"
                            elif "acne" in bad_for_lower or "ì—¬ë“œë¦„" in bad_for_list:
                                caution_reason = "ì—¬ë“œë¦„ì„± í”¼ë¶€ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤"
                            elif "acne-prone" in bad_for_lower or "ì—¬ë“œë¦„ì„±" in bad_for_list:
                                caution_reason = "ì—¬ë“œë¦„ì„± í”¼ë¶€ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤"
                            elif "allergy" in bad_for_lower or "ì•Œë ˆë¥´ê¸°" in bad_for_list:
                                caution_reason = "ì•Œë ˆë¥´ê¸° ë°˜ì‘ì„ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
                            elif "irritation" in bad_for_lower or "ìê·¹" in bad_for_list:
                                caution_reason = "í”¼ë¶€ ìê·¹ì„ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
                            
                            short_desc = description[:100] + "..." if len(description) > 100 else description
                            if short_desc:
                                bad_matches.append({
                                    "name": info.get("name", ingredient_name),
                                    "description": f"{short_desc} ({caution_reason or 'ì¼ë°˜ì ì¸ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤'})"
                                })
                            else:
                                bad_matches.append({
                                    "name": info.get("name", ingredient_name),
                                    "description": caution_reason or "ì¼ë°˜ì ì¸ ì£¼ì˜ê°€ í•„ìš”í•œ ì„±ë¶„ì…ë‹ˆë‹¤."
                                })
                            bad_ingredient_names.append(info.get("name", ingredient_name))
            
            # ì„±ë¶„ ëª©ì  ì§‘ê³„
            from collections import Counter
            all_purposes = []
            for info in ingredient_info_map.values():
                purpose = info.get("purpose", "")
                if purpose:
                    if isinstance(purpose, list):
                        all_purposes.extend(purpose)
                    elif isinstance(purpose, str):
                        all_purposes.extend([p.strip() for p in purpose.split(',') if p.strip()])
            
            purpose_counts = Counter(all_purposes)
            common_purposes_str = ", ".join([f"{p} ({c}íšŒ)" for p, c in purpose_counts.most_common(3)])
            
            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            ingredients_str = ", ".join(ingredients)
            good_ingredients_str = ", ".join(good_ingredient_names) if good_ingredient_names else "ì—†ìŒ"
            bad_ingredients_str = ", ".join(bad_ingredient_names) if bad_ingredient_names else "ì—†ìŒ"
            
            good_matches_str = ""
            if good_matches:
                good_matches_parts = []
                for match in good_matches:
                    good_matches_parts.append(f"- {match['name']}: {match['purpose']}")
                good_matches_str = "\n".join(good_matches_parts)
            else:
                good_matches_str = "ì—†ìŒ"
            
            bad_matches_str = ""
            if bad_matches:
                bad_matches_parts = []
                for match in bad_matches:
                    bad_matches_parts.append(f"- {match['name']}: {match['description']}")
                bad_matches_str = "\n".join(bad_matches_parts)
            else:
                bad_matches_str = "ì—†ìŒ"
            
            # ì¢…í•© ë¶„ì„ í”„ë¡¬í”„íŠ¸
            analysis_prompt = f"""ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í™”ì¥í’ˆ ì„±ë¶„ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìë¥¼ ìœ„í•œ 'AI ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸'ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ì„œìˆ í˜• ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ì§€ì‹œ ì‚¬í•­]
1. [ì£¼ìš” ì„±ë¶„ ëª©ì ]ì„ ë³´ê³  ì´ ì œí’ˆì˜ í•µì‹¬ ëª©ì ê³¼ ì œí’ˆ íƒ€ì…ì„ ì¶”ë¡ í•˜ì—¬ "ì´ í™”ì¥í’ˆì€(ëŠ”) [ì œí’ˆ íƒ€ì…/ëª©ì ]ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤." ë˜ëŠ” "ì´ í™”ì¥í’ˆì€(ëŠ”) [í•µì‹¬ ëª©ì ]ì— ì¤‘ì ì„ ë‘” ì œí’ˆìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."ë¡œ ë¬¸ì¥ì„ ì‹œì‘í•˜ì„¸ìš”.
2. [ì¢‹ì€ ì„±ë¶„ ëª©ë¡]ê³¼ [ì¢‹ì€ ì„±ë¶„ ì„¸ë¶€ì •ë³´]ê°€ ìˆìœ¼ë©´, "ì´ ì œí’ˆì—ëŠ” [ì„±ë¶„ëª… 1], [ì„±ë¶„ëª… 2] ë“±ì´ í¬í•¨ë˜ì–´ ìˆì–´ [ì£¼ìš” íš¨ëŠ¥]ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤." í˜•ì‹ìœ¼ë¡œ ê¸ì •ì ì¸ ë¶€ë¶„ì„ ìš”ì•½í•˜ì„¸ìš”.
3. [ì£¼ì˜ ì„±ë¶„ ëª©ë¡]ê³¼ [ì£¼ì˜ ì„±ë¶„ ì„¸ë¶€ì •ë³´]ê°€ ìˆìœ¼ë©´, "ë‹¤ë§Œ, [ì„±ë¶„ëª… 1] ë“±ì€ [ì£¼ì˜ ì´ìœ ]ì„ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë‹ˆ [ì ìš© ëŒ€ìƒ] ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤." í˜•ì‹ìœ¼ë¡œ ì£¼ì˜ì ì„ ìš”ì•½í•˜ì„¸ìš”.
4. ì œí’ˆ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.
5. ê¸ì •/ì£¼ì˜ ì„±ë¶„ì´ ì—†ë‹¤ë©´, í•´ë‹¹ ë¶€ë¶„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìƒëµí•˜ê±°ë‚˜ "íŠ¹ë³„íˆ ê¸ì •ì ì¸/ì£¼ì˜ê°€ í•„ìš”í•œ ì„±ë¶„ì€ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤."ë¼ê³  ì–¸ê¸‰í•˜ì„¸ìš”.

[ë°ì´í„°]
- ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…: {skin_type}
- ì „ì²´ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸: {ingredients_str}
- [{skin_type}]ì— ì¢‹ì€ ì„±ë¶„ ëª©ë¡: {good_ingredients_str}
- [{skin_type}]ì— ì¢‹ì€ ì„±ë¶„ ì„¸ë¶€ì •ë³´:
{good_matches_str}
- ì£¼ì˜ ì„±ë¶„ ëª©ë¡ (ì¼ë°˜ì  í¬í•¨): {bad_ingredients_str}
- ì£¼ì˜ ì„±ë¶„ ì„¸ë¶€ì •ë³´:
{bad_matches_str}
- ì°¸ê³ ìš© (ì£¼ìš” ì„±ë¶„ ëª©ì ): {common_purposes_str}

[ë¦¬í¬íŠ¸ ì‘ì„± ì‹œì‘]
"""
            
            # 4. ë‹µë³€ ìƒì„±
            analysis_report = self.llm.invoke(analysis_prompt)
            
            # 5. ìµœì¢… ì‘ë‹µ
            return {
                "analysis_report": analysis_report,
                "good_matches": good_matches,
                "bad_matches": bad_matches,
                "success": True
            }
            
        except Exception as e:
            return {
                "analysis_report": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "good_matches": [],
                "bad_matches": [],
                "success": False
            }


# ============================================================================
# FastAPI ì•± ì´ˆê¸°í™” ë° ë¼ìš°íŠ¸ ì •ì˜
# ============================================================================

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="í™”ì¥í’ˆ ì„±ë¶„ RAG API",
    description="LangChain, ChromaDB, Few-shot í”„ë¡¬í”„íŒ…ì„ í™œìš©í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ RAG ì‹œìŠ¤í…œ",
    version="2.0.0"
)

# CORS ì„¤ì • (Android ì•±ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” êµ¬ì²´ì ì¸ ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
print("ğŸš€ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
ingredients_file = os.path.join(project_root, 'app', 'src', 'main', 'assets', 'ingredients.json')
rag_system = EnterpriseRAG(ingredients_file)


@app.get("/", tags=["Root"])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "í™”ì¥í’ˆ ì„±ë¶„ RAG API ì„œë²„",
        "version": "2.0.0 (FastAPI)",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return HealthResponse(
        status="healthy",
        message="ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ RAG ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤ (FastAPI)",
        ingredients_count=len(rag_system.ingredients_data),
        features=[
            "FastAPI Framework",
            "LangChain Integration",
            "ChromaDB Vector Store",
            "Chat History Management",
            "Few-shot Prompting",
            "Enterprise-grade RAG Pipeline",
            "Async Processing"
        ]
    )


@app.post("/search", response_model=SearchResponse, tags=["Search"])
async def search_ingredients(request: SearchRequest):
    """LangChain ê¸°ë°˜ ì„±ë¶„ ê²€ìƒ‰ API (ì±„íŒ… íˆìŠ¤í† ë¦¬ ì§€ì›)"""
    if not request.query:
        raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
    
    try:
        result = rag_system.search_ingredients(request.query, request.session_id)
        return SearchResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


@app.post("/analyze_product", response_model=AnalyzeProductResponse, tags=["Analysis"])
async def analyze_product(request: AnalyzeProductRequest):
    """
    ì œí’ˆ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ì™€ í”¼ë¶€ íƒ€ì…ì„ ê¸°ë°˜ìœ¼ë¡œ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± API
    """
    if not request.ingredients or not isinstance(request.ingredients, list):
        raise HTTPException(status_code=400, detail="ingredients í•„ë“œëŠ” ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    
    if not request.skin_type or not isinstance(request.skin_type, str):
        raise HTTPException(status_code=400, detail="skin_type í•„ë“œëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    
    try:
        result = rag_system.analyze_product_ingredients(request.ingredients, request.skin_type)
        
        # GoodMatch, BadMatch ê°ì²´ë¡œ ë³€í™˜
        good_matches_obj = [GoodMatch(**match) for match in result["good_matches"]]
        bad_matches_obj = [BadMatch(**match) for match in result["bad_matches"]]
        
        return AnalyzeProductResponse(
            analysis_report=result["analysis_report"],
            good_matches=good_matches_obj,
            bad_matches=bad_matches_obj,
            success=result["success"]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


@app.get("/chat/history", response_model=ChatHistoryResponse, tags=["Chat"])
async def get_chat_history(session_id: str = Query(..., description="ì„¸ì…˜ ID")):
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ API"""
    if not session_id:
        raise HTTPException(status_code=400, detail="session_idê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    try:
        result = rag_system.get_chat_history(session_id)
        if "error" in result:
            raise HTTPException(status_code=404, detail=result["error"])
        return ChatHistoryResponse(**result)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


@app.post("/chat/clear", tags=["Chat"])
async def clear_chat_history(request: ClearHistoryRequest):
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” API"""
    if not request.session_id:
        raise HTTPException(status_code=400, detail="session_idê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    try:
        result = rag_system.clear_chat_history(request.session_id)
        if "error" in result:
            raise HTTPException(status_code=404, detail=result["error"])
        return result
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


@app.get("/ingredients", tags=["Ingredients"])
async def get_all_ingredients():
    """ëª¨ë“  ì„±ë¶„ ëª©ë¡ ì¡°íšŒ"""
    try:
        ingredients = []
        for item in rag_system.ingredients_data:
            kor_name = item.get('INGR_KOR_NAME', '')
            eng_name = item.get('INGR_ENG_NAME', '')
            if kor_name and eng_name:
                ingredients.append(f"{kor_name} ({eng_name})")
            elif kor_name:
                ingredients.append(kor_name)
            elif eng_name:
                ingredients.append(eng_name)
        return {
            'ingredients': ingredients,
            'count': len(ingredients),
            'success': True
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


@app.get("/sessions", tags=["Sessions"])
async def get_active_sessions():
    """í™œì„± ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
    try:
        sessions = []
        for session_id, memory in rag_system.chat_sessions.items():
            sessions.append({
                'session_id': session_id,
                'message_count': len(memory.messages),
                'last_activity': datetime.now().isoformat()
            })
        
        return {
            'sessions': sessions,
            'total_sessions': len(sessions),
            'success': True
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


@app.get("/rag/info", tags=["RAG System"])
async def get_rag_info():
    """RAG ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
    try:
        return {
            'system_type': 'Enterprise RAG Pipeline',
            'framework': 'FastAPI',
            'components': {
                'vector_store': 'ChromaDB',
                'embedding_model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                'llm': 'MockLLM (Demo)',
                'framework': 'LangChain',
                'memory': 'ConversationBufferMemory',
                'prompting': 'Few-shot Prompting'
            },
            'features': [
                'Async Processing',
                'Vector Database Integration',
                'Chat History Management',
                'Few-shot Learning',
                'Context-aware Responses',
                'Session Management',
                'Auto API Documentation'
            ],
            'success': True
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


if __name__ == '__main__':
    import uvicorn
    
    print("ğŸš€ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ í™”ì¥í’ˆ ì„±ë¶„ RAG ì±—ë´‡ ì„œë²„ ì‹œì‘ (FastAPI)")
    print("ğŸ“± Android ì•±ì—ì„œ http://localhost:5000 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”")
    print("ğŸ“š API ë¬¸ì„œ: http://localhost:5000/docs (Swagger UI)")
    print("ğŸ“Š ëŒ€ì²´ ë¬¸ì„œ: http://localhost:5000/redoc (ReDoc)")
    print("ğŸ”§ ì£¼ìš” ê¸°ëŠ¥:")
    print("  - FastAPI í”„ë ˆì„ì›Œí¬ (ë¹„ë™ê¸° ì²˜ë¦¬)")
    print("  - LangChain ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸")
    print("  - ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤")
    print("  - ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬")
    print("  - Few-shot í”„ë¡¬í”„íŒ…")
    print("  - ì„¸ì…˜ ê´€ë¦¬")
    print("  - ì œí’ˆ ì¢…í•© ë¶„ì„ ê¸°ëŠ¥")
    print("  - ìë™ API ë¬¸ì„œí™”")
    print("ğŸ“Š API ì—”ë“œí¬ì¸íŠ¸:")
    print("  - GET  / - ë£¨íŠ¸")
    print("  - GET  /health - ì„œë²„ ìƒíƒœ í™•ì¸")
    print("  - POST /search - ì„±ë¶„ ê²€ìƒ‰ (ì±„íŒ… íˆìŠ¤í† ë¦¬ ì§€ì›)")
    print("  - POST /analyze_product - ì œí’ˆ ì„±ë¶„ ì¢…í•© ë¶„ì„")
    print("  - GET  /chat/history?session_id=<id> - ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ")
    print("  - POST /chat/clear - ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")
    print("  - GET  /sessions - í™œì„± ì„¸ì…˜ ëª©ë¡")
    print("  - GET  /ingredients - ì „ì²´ ì„±ë¶„ ëª©ë¡")
    print("  - GET  /rag/info - RAG ì‹œìŠ¤í…œ ì •ë³´")
    
    uvicorn.run(app, host="0.0.0.0", port=5000, log_level="info")

