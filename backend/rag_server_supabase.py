#!/usr/bin/env python3
"""
í™”ì¥í’ˆ ì„±ë¶„ RAG ì±—ë´‡ ì„œë²„ - Supabase PostgreSQL ë²„ì „
ê¸°ì¡´ rag_server_fastapi.pyë¥¼ Supabaseì™€ ì—°ë™í•˜ë„ë¡ ìˆ˜ì •
Supabase ì—°ê²° ì‹¤íŒ¨ ì‹œ JSON íŒŒì¼ë¡œ í´ë°±
"""

import json
import logging
import os
import uuid
from typing import List, Dict, Optional, Any
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ê´€ë ¨ imports
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# LangChain ê´€ë ¨ imports
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun

# Supabase í´ë¼ì´ì–¸íŠ¸
from supabase_client import (
    is_supabase_available,
    get_ingredients_by_names,
    get_all_ingredients,
    get_ingredients_count,
    search_ingredients as supabase_search_ingredients,
    test_supabase_connection
)


# ============================================================================
# Pydantic ëª¨ë¸ ì •ì˜
# ============================================================================

class SearchRequest(BaseModel):
    """
    ì„±ë¶„ ê²€ìƒ‰ ìš”ì²­ ëª¨ë¸
    
    Attributes:
        query: ê²€ìƒ‰í•  ì„±ë¶„ëª… ë˜ëŠ” ì§ˆë¬¸
        session_id: ì±„íŒ… ì„¸ì…˜ ID (ì„ íƒì , ì—†ìœ¼ë©´ ìë™ ìƒì„±)
    """
    query: str = Field(..., description="ê²€ìƒ‰í•  ì„±ë¶„ëª… ë˜ëŠ” ì§ˆë¬¸")
    session_id: Optional[str] = Field(None, description="ì±„íŒ… ì„¸ì…˜ ID")


class AnalyzeProductRequest(BaseModel):
    """
    ì œí’ˆ ë¶„ì„ ìš”ì²­ ëª¨ë¸
    
    Attributes:
        ingredients: ë¶„ì„í•  ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸
        skin_type: ì‚¬ìš©ì í”¼ë¶€ íƒ€ì… (ì˜ˆ: "ê±´ì„±, ë¯¼ê°ì„±")
    """
    ingredients: List[str] = Field(..., description="ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸")
    skin_type: str = Field(..., description="ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…")


class GoodMatch(BaseModel):
    """
    ì¢‹ì€ ì„±ë¶„ ë§¤ì¹­ ê²°ê³¼ ëª¨ë¸
    
    Attributes:
        name: ì„±ë¶„ëª…
        purpose: ì„±ë¶„ì˜ ëª©ì /ê¸°ëŠ¥ (ì˜ë¬¸ ë˜ëŠ” í•œêµ­ì–´)
    """
    name: str
    purpose: str


class BadMatch(BaseModel):
    """
    ì£¼ì˜ ì„±ë¶„ ë§¤ì¹­ ê²°ê³¼ ëª¨ë¸
    
    Attributes:
        name: ì„±ë¶„ëª…
        description: ì„±ë¶„ì— ëŒ€í•œ ì„¤ëª… (ì˜ë¬¸, í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë²ˆì—­ë¨)
    """
    name: str
    description: str


class AnalyzeProductResponse(BaseModel):
    """
    ì œí’ˆ ë¶„ì„ ì‘ë‹µ ëª¨ë¸
    
    Attributes:
        analysis_report: AIê°€ ìƒì„±í•œ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ (í•œêµ­ì–´)
        good_matches: ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…ì— ì¢‹ì€ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
        bad_matches: ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…ì— ì£¼ì˜ê°€ í•„ìš”í•œ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
        success: ë¶„ì„ ì„±ê³µ ì—¬ë¶€
    """
    analysis_report: str
    good_matches: List[GoodMatch]
    bad_matches: List[BadMatch]
    success: bool


class SearchResponse(BaseModel):
    """
    ì„±ë¶„ ê²€ìƒ‰ ì‘ë‹µ ëª¨ë¸
    
    Attributes:
        query: ê²€ìƒ‰í•œ ì¿¼ë¦¬
        answer: ê²€ìƒ‰ ê²°ê³¼ ë‹µë³€
        similar_ingredients: ìœ ì‚¬í•œ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
        session_id: ì±„íŒ… ì„¸ì…˜ ID
        chat_history: ìµœê·¼ ì±„íŒ… íˆìŠ¤í† ë¦¬ (ìµœëŒ€ 4ê°œ)
        success: ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€
    """
    query: str
    answer: str
    similar_ingredients: List[Dict[str, Any]]
    session_id: str
    chat_history: List[Dict[str, str]]
    success: bool


class HealthResponse(BaseModel):
    """
    ì„œë²„ ìƒíƒœ í™•ì¸ ì‘ë‹µ ëª¨ë¸
    
    Attributes:
        status: ì„œë²„ ìƒíƒœ ("healthy")
        message: ìƒíƒœ ë©”ì‹œì§€
        ingredients_count: ì €ì¥ëœ ì„±ë¶„ ê°œìˆ˜
        features: ì§€ì›í•˜ëŠ” ê¸°ëŠ¥ ë¦¬ìŠ¤íŠ¸
        database: ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°ë² ì´ìŠ¤ ("supabase" or "json")
    """
    status: str
    message: str
    ingredients_count: int
    features: List[str]
    database: str  # "supabase" or "json"


# ============================================================================
# ë©”ëª¨ë¦¬ ë° LLM í´ë˜ìŠ¤
# ============================================================================

class SimpleConversationMemory:
    """
    ê°„ë‹¨í•œ ëŒ€í™” ë©”ëª¨ë¦¬ í´ë˜ìŠ¤
    
    ì±„íŒ… ì„¸ì…˜ì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
    ê° ë©”ì‹œì§€ëŠ” ì…ë ¥, ì¶œë ¥, íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
    
    Attributes:
        messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    """
    def __init__(self):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        self.messages = []
    
    def save_context(self, inputs: Dict, outputs: Dict):
        """
        ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            inputs: ì…ë ¥ ë”•ì…”ë„ˆë¦¬ (ì˜ˆ: {'input': 'ì§ˆë¬¸'})
            outputs: ì¶œë ¥ ë”•ì…”ë„ˆë¦¬ (ì˜ˆ: {'output': 'ë‹µë³€'})
        """
        self.messages.append({
            'input': inputs.get('input', ''),
            'output': outputs.get('output', ''),
            'timestamp': datetime.now().isoformat()
        })
    
    def clear(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤."""
        self.messages.clear()
    
    @property
    def chat_memory(self):
        """
        ì±„íŒ… ë©”ëª¨ë¦¬ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            ìê¸° ìì‹  (LangChain í˜¸í™˜ì„±ì„ ìœ„í•´)
        """
        return self


class MockLLM(LLM):
    """
    Mock LLM í´ë˜ìŠ¤ - ì œí’ˆ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    
    ì‹¤ì œ LLM ëŒ€ì‹  ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì œí’ˆ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    LangChainì˜ LLM ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬ RAG íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    
    ìƒì„± ë¡œì§:
    - í”„ë¡¬í”„íŠ¸ì—ì„œ í”¼ë¶€ íƒ€ì…, ì¢‹ì€ ì„±ë¶„, ì£¼ì˜ ì„±ë¶„ì„ ì¶”ì¶œ
    - ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¦¬í¬íŠ¸ ìƒì„±
    """
    
    @property
    def _llm_type(self) -> str:
        """
        LLM íƒ€ì…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            "mock" (Mock LLMì„ì„ ë‚˜íƒ€ëƒ„)
        """
        return "mock"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        í˜„ì¬ëŠ” "ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸" ìƒì„±ë§Œ ì§€ì›í•©ë‹ˆë‹¤.
        ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ëŠ” ê¸°ë³¸ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            stop: ì¤‘ì§€ í† í° ë¦¬ìŠ¤íŠ¸ (ì‚¬ìš© ì•ˆ í•¨)
            run_manager: ì½œë°± ë§¤ë‹ˆì € (ì‚¬ìš© ì•ˆ í•¨)
            **kwargs: ì¶”ê°€ ì¸ì
        
        Returns:
            ìƒì„±ëœ ë¦¬í¬íŠ¸ ë¬¸ìì—´
        """
        import re
        
        if "ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸" in prompt or "ì¢…í•© ë¶„ì„" in prompt:
            return self._generate_product_analysis(prompt)
        
        return "í•´ë‹¹ ì„±ë¶„ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _generate_product_analysis(self, prompt: str) -> str:
        """
        ì œí’ˆ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        í”„ë¡¬í”„íŠ¸ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:
        - ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…
        - ì¢‹ì€ ì„±ë¶„ ëª©ë¡
        - ì£¼ì˜ ì„±ë¶„ ëª©ë¡
        - ì£¼ìš” ì„±ë¶„ ëª©ì 
        
        ë¦¬í¬íŠ¸ êµ¬ì¡°:
        1. ì œí’ˆ íƒ€ì… ì¶”ë¡  (ë³´ìŠµ, í•­ì‚°í™”, ê°ì§ˆ ì œê±° ë“±)
        2. ê¸ì •ì  ë¶„ì„ (ì¢‹ì€ ì„±ë¶„ ì–¸ê¸‰)
        3. ì£¼ì˜ ì„±ë¶„ ë¶„ì„
        4. ì¢…í•© í‰ê°€
        
        Args:
            prompt: ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± í”„ë¡¬í”„íŠ¸
        
        Returns:
            ìƒì„±ëœ ë¶„ì„ ë¦¬í¬íŠ¸ (í•œêµ­ì–´)
        """
        import re
        
        # í”¼ë¶€ íƒ€ì… ì¶”ì¶œ
        skin_type_match = re.search(r'ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…:\s*([^\n]+)', prompt)
        skin_type = skin_type_match.group(1).strip() if skin_type_match else "ì•Œ ìˆ˜ ì—†ëŠ”"
        
        # ì¢‹ì€ ì„±ë¶„ ëª©ë¡ ì¶”ì¶œ
        good_match_str = re.search(r'\[.*?\]ì— ì¢‹ì€ ì„±ë¶„ ëª©ë¡:\s*([^\n]+)', prompt)
        if not good_match_str:
            good_match_str = re.search(r'ì¢‹ì€ ì„±ë¶„ ëª©ë¡:\s*([^\n]+)', prompt)
        good_names = good_match_str.group(1).strip() if good_match_str else ""
        if good_names == "ì—†ìŒ":
            good_names = ""
        
        # ì£¼ì˜ ì„±ë¶„ ëª©ë¡ ì¶”ì¶œ
        bad_match_str = re.search(r'ì£¼ì˜ ì„±ë¶„ ëª©ë¡ \(ì¼ë°˜ì  í¬í•¨\):\s*([^\n]+)', prompt)
        if not bad_match_str:
            bad_match_str = re.search(r'ì£¼ì˜ ì„±ë¶„ ëª©ë¡:\s*([^\n]+)', prompt)
        bad_names = bad_match_str.group(1).strip() if bad_match_str else ""
        if bad_names == "ì—†ìŒ":
            bad_names = ""
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report_parts = []
        
        # ì œí’ˆ íƒ€ì… ì¶”ë¡ 
        purpose_match = re.search(r'ì£¼ìš” ì„±ë¶„ ëª©ì \):\s*([^\n]+)', prompt)
        main_purpose = "ë³µí•©ì ì¸"
        
        if purpose_match:
            purposes = purpose_match.group(1).strip()
            purpose_map = {
                "moisturizer": "ë³´ìŠµ", "antioxidant": "í•­ì‚°í™”",
                "exfoliant": "ê°ì§ˆ ì œê±°", "fragrance": "í–¥ë£Œ",
                "preservative": "ë³´ì¡´", "emulsifier": "ìœ í™”"
            }
            first_purpose_match = re.search(r'([a-zA-Zê°€-í£\s]+)\s*\(\d+íšŒ\)', purposes)
            if first_purpose_match:
                purpose_name = first_purpose_match.group(1).strip().lower()
                main_purpose = purpose_map.get(purpose_name, purpose_name)
        
        report_parts.append(f"ì´ í™”ì¥í’ˆì€(ëŠ”) '{main_purpose}'ì— ì¤‘ì ì„ ë‘” ì œí’ˆìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.")
        
        # ê¸ì •ì  ë¶„ì„
        if good_names:
            good_names_list = [n.strip() for n in good_names.split(',') if n.strip()][:3]
            good_names_short = ", ".join(good_names_list)
            if len([n.strip() for n in good_names.split(',') if n.strip()]) > 3:
                good_names_short += " ë“±"
            report_parts.append(f"íŠ¹íˆ {skin_type} í”¼ë¶€ì— ì¢‹ì€ {good_names_short} ì„±ë¶„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # ì£¼ì˜ ì„±ë¶„ ë¶„ì„
        if bad_names:
            bad_names_list = [n.strip() for n in bad_names.split(',') if n.strip()][:2]
            bad_names_short = ", ".join(bad_names_list)
            report_parts.append(f"ë‹¤ë§Œ, {bad_names_short} ì„±ë¶„ì€ ì¼ë¶€ í”¼ë¶€ì— ìê·¹ì„ ì¤„ ìˆ˜ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì„¸ìš”.")
        
        # ì¢…í•© í‰ê°€
        if good_names and not bad_names:
            report_parts.append(f"ì „ë°˜ì ìœ¼ë¡œ {skin_type} í”¼ë¶€ì— ì¢‹ì€ ì œí’ˆìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.")
        elif good_names and bad_names:
            report_parts.append(f"ì‚¬ìš© ì‹œ í”¼ë¶€ ë°˜ì‘ì„ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
        else:
            report_parts.append(f"ê°œì¸ì ì¸ í”¼ë¶€ ë°˜ì‘ì„ í™•ì¸í•˜ë©° ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.")
        
        return " ".join(report_parts)


# ============================================================================
# EnterpriseRAG í´ë˜ìŠ¤ (Supabase í†µí•©)
# ============================================================================

class EnterpriseRAG:
    """
    ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤
    
    Supabase PostgreSQLê³¼ ChromaDBë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
    Supabase ì—°ê²° ì‹¤íŒ¨ ì‹œ JSON íŒŒì¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ì„±ë¶„ ê²€ìƒ‰ (Supabase ì§ì ‘ ê²€ìƒ‰ ë˜ëŠ” ë²¡í„° ê²€ìƒ‰)
    - ì œí’ˆ ì„±ë¶„ ë¶„ì„ (í”¼ë¶€ íƒ€ì… ê¸°ë°˜)
    - ì±„íŒ… ì„¸ì…˜ ê´€ë¦¬
    
    ë°ì´í„° ì†ŒìŠ¤:
    - ìš°ì„ ìˆœìœ„ 1: Supabase PostgreSQL
    - ìš°ì„ ìˆœìœ„ 2: JSON íŒŒì¼ (í´ë°±)
    
    ë²¡í„° ìŠ¤í† ì–´:
    - ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ë¶„ ì •ë³´ë¥¼ ì„ë² ë”©í•˜ì—¬ ì €ì¥
    - ë‹¤êµ­ì–´ ì§€ì› (paraphrase-multilingual-MiniLM-L12-v2)
    
    Args:
        data_file: JSON í´ë°± íŒŒì¼ ê²½ë¡œ
        persist_directory: ChromaDB ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    
    def __init__(self, data_file: str, persist_directory: str = "./chroma_db_ingredients"):
        self.data_file = data_file
        self.persist_directory = persist_directory
        self.ingredients_data = []
        self.use_supabase = False
        
        # LangChain ì»´í¬ë„ŒíŠ¸
        self.text_splitter = None
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.retriever = None
        
        # Chat History ê´€ë¦¬
        self.chat_sessions = {}
        
        # ì´ˆê¸°í™”
        self._initialize()
    
    def _initialize(self):
        """
        RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        ì´ˆê¸°í™” ê³¼ì •:
        1. Supabase ì—°ê²° í™•ì¸
        2. ë°ì´í„° ë¡œë“œ (Supabase ë˜ëŠ” JSON)
        3. LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        4. ChromaDB ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        
        Raises:
            Exception: ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
        """
        logger.info("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # Supabase ì—°ê²° í™•ì¸
        if is_supabase_available():
            logger.info("âœ… Supabase ì—°ê²° ì„±ê³µ!")
            self.use_supabase = True
            self.ingredients_data = get_all_ingredients()
            logger.info(f"ğŸ“Š Supabaseì—ì„œ {len(self.ingredients_data)}ê°œ ì„±ë¶„ ë¡œë“œ")
        else:
            logger.warning("âš ï¸ Supabase ì—°ê²° ì‹¤íŒ¨, JSON íŒŒì¼ ì‚¬ìš©")
            self.use_supabase = False
            self._load_json_data()
        
        # LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_langchain()
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        self._create_vectorstore()
    
    def _load_json_data(self):
        """
        JSON íŒŒì¼ì—ì„œ ì„±ë¶„ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Supabase ì—°ê²° ì‹¤íŒ¨ ì‹œ í´ë°±ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        JSON í˜•ì‹ì„ Supabase í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        
        ì²˜ë¦¬ ê³¼ì •:
        1. JSON íŒŒì¼ ì½ê¸°
        2. ê° í•­ëª©ì„ Supabase í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        3. ingredients_data ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        
        ë³€í™˜ ê·œì¹™:
        - INGR_KOR_NAME â†’ kor_name
        - INGR_ENG_NAME â†’ eng_name
        - description â†’ description
        - purpose â†’ purpose (ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        - good_for â†’ good_for (ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        - bad_for â†’ bad_for (ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        
        Raises:
            Exception: JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ
        """
        logger.info("ğŸ“š JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            # Supabase í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            self.ingredients_data = []
            for item in raw_data:
                self.ingredients_data.append({
                    "kor_name": item.get("INGR_KOR_NAME", ""),
                    "eng_name": item.get("INGR_ENG_NAME", ""),
                    "description": item.get("description", ""),
                    "purpose": item.get("purpose") or [],
                    "good_for": item.get("good_for") or [],
                    "bad_for": item.get("bad_for") or []
                })
            
            logger.info(f"âœ… {len(self.ingredients_data)}ê°œ ì„±ë¶„ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ JSON ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
            self.ingredients_data = []
    
    def _initialize_langchain(self):
        """
        LangChain ì»´í¬ë„ŒíŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        ì´ˆê¸°í™”í•˜ëŠ” ì»´í¬ë„ŒíŠ¸:
        - RecursiveCharacterTextSplitter: í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        - SentenceTransformerEmbeddings: ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸
        - MockLLM: ë¦¬í¬íŠ¸ ìƒì„±ìš© LLM
        
        ì„¤ì •:
        - chunk_size: 1000ì
        - chunk_overlap: 200ì (ì²­í¬ ê°„ ê²¹ì¹¨)
        - embedding ëª¨ë¸: paraphrase-multilingual-MiniLM-L12-v2 (í•œêµ­ì–´ ì§€ì›)
        """
        logger.info("ğŸ”§ LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=200
        )
        
        self.embeddings = SentenceTransformerEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        
        self.llm = MockLLM()
        logger.info("âœ… LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _create_vectorstore(self):
        """
        ChromaDB ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        ê° ì„±ë¶„ ì •ë³´ë¥¼ Documentë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥í•©ë‹ˆë‹¤.
        
        ì²˜ë¦¬ ê³¼ì •:
        1. ê° ì„±ë¶„ ì •ë³´ë¥¼ Documentë¡œ ë³€í™˜
           - page_content: ì„±ë¶„ ì •ë³´ í…ìŠ¤íŠ¸ (í•œêµ­ì–´ëª…, ì˜ì–´ëª…, ì„¤ëª…, ëª©ì  ë“±)
           - metadata: êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„° (ì„±ë¶„ëª…, ì„¤ëª…, ëª©ì , í”¼ë¶€ íƒ€ì… ë“±)
        2. í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (RecursiveCharacterTextSplitter ì‚¬ìš©)
        3. ChromaDBì— ì €ì¥ ë° ì„ë² ë”© ìƒì„±
        4. Retriever ìƒì„± (top_k=3)
        
        ë²¡í„° ê²€ìƒ‰:
        - ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        - ìƒìœ„ 3ê°œ ê²°ê³¼ ë°˜í™˜
        
        Raises:
            Exception: ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨ ì‹œ
        """
        logger.info("ğŸ—„ï¸ ChromaDB ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        
        documents = []
        for item in self.ingredients_data:
            kor_name = item.get('kor_name', '')
            eng_name = item.get('eng_name', '')
            description = item.get('description', '')
            purpose = item.get('purpose', [])
            good_for = item.get('good_for', [])
            bad_for = item.get('bad_for', [])
            
            content_parts = []
            if kor_name:
                content_parts.append(f"í•œêµ­ì–´ ì„±ë¶„ëª…: {kor_name}")
            if eng_name:
                content_parts.append(f"ì˜ì–´ ì„±ë¶„ëª…: {eng_name}")
            if description:
                content_parts.append(f"ì„¤ëª…: {description[:500]}")
            if purpose:
                content_parts.append(f"ëª©ì : {', '.join(purpose) if isinstance(purpose, list) else purpose}")
            if good_for:
                content_parts.append(f"ê¶Œì¥ í”¼ë¶€ íƒ€ì…: {', '.join(good_for) if isinstance(good_for, list) else good_for}")
            if bad_for:
                content_parts.append(f"ì£¼ì˜ í”¼ë¶€ íƒ€ì…: {', '.join(bad_for) if isinstance(bad_for, list) else bad_for}")
            
            content = "\n".join(content_parts)
            
            doc = Document(
                page_content=content,
                metadata={
                    "ingredient_kor": kor_name,
                    "ingredient_eng": eng_name,
                    "description": (description[:200] + "..." if description and len(description) > 200 else description) or '',
                    "purpose": ', '.join(purpose) if isinstance(purpose, list) else (purpose or ''),
                    "good_for": ', '.join(good_for) if isinstance(good_for, list) else (good_for or ''),
                    "bad_for": ', '.join(bad_for) if isinstance(bad_for, list) else (bad_for or '')
                }
            )
            documents.append(doc)
        
        split_docs = self.text_splitter.split_documents(documents)
        logger.info(f"ğŸ“„ {len(documents)}ê°œ ë¬¸ì„œë¥¼ {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        self.vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        
        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 3})
        logger.info("âœ… ChromaDB ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
    
    def get_data_source(self) -> str:
        """
        í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            "supabase" ë˜ëŠ” "json"
        """
        return "supabase" if self.use_supabase else "json"
    
    def get_ingredients_count(self) -> int:
        """
        ì €ì¥ëœ ì„±ë¶„ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            ì„±ë¶„ ê°œìˆ˜
        """
        if self.use_supabase:
            return get_ingredients_count()
        return len(self.ingredients_data)
    
    def get_or_create_session(self, session_id: str = None) -> str:
        """
        ì±„íŒ… ì„¸ì…˜ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            session_id: ì„¸ì…˜ ID (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
        
        Returns:
            ì„¸ì…˜ ID
        """
        if session_id is None:
            session_id = str(uuid.uuid4())
        if session_id not in self.chat_sessions:
            self.chat_sessions[session_id] = SimpleConversationMemory()
        return session_id
    
    def search_ingredients(self, query: str, session_id: str = None, top_k: int = 3) -> Dict:
        """
        ì„±ë¶„ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        ê²€ìƒ‰ ë°©ì‹:
        1. Supabase ì‚¬ìš© ì‹œ: ì§ì ‘ SQL ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ (ë¹ ë¦„)
        2. Supabase ë¯¸ì‚¬ìš© ì‹œ: ChromaDB ë²¡í„° ê²€ìƒ‰ (í´ë°±)
        
        ë°˜í™˜ ì •ë³´:
        - ê²€ìƒ‰ ê²°ê³¼ ë‹µë³€
        - ìœ ì‚¬í•œ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ top_kê°œ)
        - ì±„íŒ… íˆìŠ¤í† ë¦¬ (ìµœê·¼ 4ê°œ)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì„±ë¶„ëª… ë˜ëŠ” ì§ˆë¬¸)
            session_id: ì±„íŒ… ì„¸ì…˜ ID (ì„ íƒì )
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ê°’: 3)
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬:
            - query: ê²€ìƒ‰ ì¿¼ë¦¬
            - answer: ê²€ìƒ‰ ê²°ê³¼ ë‹µë³€
            - similar_ingredients: ìœ ì‚¬í•œ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
            - session_id: ì„¸ì…˜ ID
            - chat_history: ì±„íŒ… íˆìŠ¤í† ë¦¬
            - success: ì„±ê³µ ì—¬ë¶€
        
        Raises:
            Exception: ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ (ë”•ì…”ë„ˆë¦¬ë¡œ ê°ì‹¸ì„œ ë°˜í™˜)
        """
        session_id = self.get_or_create_session(session_id)
        memory = self.chat_sessions[session_id]
        
        try:
            # Supabase ì§ì ‘ ê²€ìƒ‰
            if self.use_supabase:
                db_results = supabase_search_ingredients(query, limit=top_k)
                if db_results:
                    first_result = db_results[0]
                    answer = f"{first_result.get('kor_name', '')}ì— ëŒ€í•œ ì •ë³´: {first_result.get('description', '')[:300]}"
                    
                    similar_ingredients = [{
                        "ingredient_kor": r.get('kor_name', ''),
                        "ingredient_eng": r.get('eng_name', ''),
                        "description": r.get('description', '')[:200],
                        "purpose": ', '.join(r.get('purpose', [])),
                        "good_for": ', '.join(r.get('good_for', [])),
                        "bad_for": ', '.join(r.get('bad_for', []))
                    } for r in db_results]
                    
                    memory.save_context({"input": query}, {"output": answer})
                    
                    return {
                        "query": query,
                        "answer": answer,
                        "similar_ingredients": similar_ingredients,
                        "session_id": session_id,
                        "chat_history": memory.messages[-4:],
                        "success": True
                    }
            
            # ë²¡í„° ê²€ìƒ‰ í´ë°±
            docs = self.retriever.invoke(query)
            if docs:
                first_doc = docs[0]
                answer = f"{first_doc.metadata.get('ingredient_kor', '')}ì— ëŒ€í•œ ì •ë³´: {first_doc.metadata.get('description', '')}"
                
                similar_ingredients = [{
                    "ingredient_kor": d.metadata.get('ingredient_kor', ''),
                    "ingredient_eng": d.metadata.get('ingredient_eng', ''),
                    "description": d.metadata.get('description', ''),
                    "purpose": d.metadata.get('purpose', ''),
                    "good_for": d.metadata.get('good_for', ''),
                    "bad_for": d.metadata.get('bad_for', '')
                } for d in docs]
                
                memory.save_context({"input": query}, {"output": answer})
                
                return {
                    "query": query,
                    "answer": answer,
                    "similar_ingredients": similar_ingredients,
                    "session_id": session_id,
                    "chat_history": memory.messages[-4:],
                    "success": True
                }
            
            return {
                "query": query,
                "answer": "í•´ë‹¹ ì„±ë¶„ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "similar_ingredients": [],
                "session_id": session_id,
                "chat_history": [],
                "success": False
            }
            
        except Exception as e:
            return {
                "query": query,
                "answer": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "similar_ingredients": [],
                "session_id": session_id,
                "chat_history": [],
                "success": False
            }
    
    def analyze_product_ingredients(self, ingredients: List[str], skin_type: str) -> Dict:
        """
        ì œí’ˆì˜ ì„±ë¶„ì„ ë¶„ì„í•©ë‹ˆë‹¤.
        
        ì‚¬ìš©ìì˜ í”¼ë¶€ íƒ€ì…ì„ ê¸°ë°˜ìœ¼ë¡œ ê° ì„±ë¶„ì´ ì¢‹ì€ ì„±ë¶„ì¸ì§€ ì£¼ì˜ ì„±ë¶„ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
        
        ë¶„ì„ ê³¼ì •:
        1. ì„±ë¶„ ì •ë³´ ì¡°íšŒ (Supabase ë˜ëŠ” ë¡œì»¬)
        2. í”¼ë¶€ íƒ€ì… ì •ê·œí™” (í•œêµ­ì–´ â†’ ì˜ë¬¸)
        3. ê° ì„±ë¶„ì— ëŒ€í•´ good_for/bad_for í™•ì¸
        4. ì„±ë¶„ ëª©ì  ì§‘ê³„ (ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” ëª©ì  ì¶”ì¶œ)
        5. MockLLMìœ¼ë¡œ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        
        íŒë‹¨ ê¸°ì¤€:
        - good_forì— ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…ì´ í¬í•¨ â†’ ì¢‹ì€ ì„±ë¶„
        - bad_forì— ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…ì´ í¬í•¨ â†’ ì£¼ì˜ ì„±ë¶„
        - bad_forì— "sensitive" ë˜ëŠ” "ë¯¼ê°ì„±" í¬í•¨ â†’ ì¼ë°˜ì ìœ¼ë¡œ ì£¼ì˜ ì„±ë¶„
        
        Args:
            ingredients: ë¶„ì„í•  ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸
            skin_type: ì‚¬ìš©ì í”¼ë¶€ íƒ€ì… (ì˜ˆ: "ê±´ì„±, ë¯¼ê°ì„±")
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬:
            - analysis_report: ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ (í•œêµ­ì–´)
            - good_matches: ì¢‹ì€ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
            - bad_matches: ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
            - success: ë¶„ì„ ì„±ê³µ ì—¬ë¶€
        
        Raises:
            Exception: ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ (ë”•ì…”ë„ˆë¦¬ë¡œ ê°ì‹¸ì„œ ë°˜í™˜)
        """
        try:
            # ì„±ë¶„ ì •ë³´ ì¡°íšŒ
            if self.use_supabase:
                ingredient_info_map = get_ingredients_by_names(ingredients)
            else:
                ingredient_info_map = self._get_ingredients_from_local(ingredients)
            
            # í”¼ë¶€ íƒ€ì… ë§¤í•‘
            skin_type_map = {
                "ê±´ì„±": "dry", "ì§€ì„±": "oily", "ë¯¼ê°ì„±": "sensitive",
                "ì—¬ë“œë¦„ì„±": "acne", "ë³µí•©ì„±": "combination", "ì¤‘ì„±": "normal"
            }
            normalized_skin_type = skin_type_map.get(skin_type, skin_type.lower())
            
            good_matches = []
            bad_matches = []
            good_names = []
            bad_names = []
            
            for ingredient_name, info in ingredient_info_map.items():
                good_for = info.get('good_for', []) or []
                bad_for = info.get('bad_for', []) or []
                purpose = info.get('purpose', []) or []
                description = info.get('description', '')
                display_name = info.get('kor_name') or info.get('eng_name') or ingredient_name
                
                # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                if isinstance(good_for, str):
                    good_for = [g.strip() for g in good_for.split(',') if g.strip()]
                if isinstance(bad_for, str):
                    bad_for = [b.strip() for b in bad_for.split(',') if b.strip()]
                if isinstance(purpose, str):
                    purpose = [p.strip() for p in purpose.split(',') if p.strip()]
                
                # good_for ë¶„ì„
                if normalized_skin_type in good_for or skin_type in good_for:
                    good_matches.append({
                        "name": display_name,
                        "purpose": ', '.join(purpose) if purpose else "ê¸°ëŠ¥ ì •ë³´ ì—†ìŒ"
                    })
                    good_names.append(display_name)
                
                # bad_for ë¶„ì„
                if normalized_skin_type in bad_for or skin_type in bad_for:
                    short_desc = description[:100] + "..." if len(description) > 100 else description
                    bad_matches.append({
                        "name": display_name,
                        "description": short_desc if short_desc else f"{skin_type} í”¼ë¶€ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                    })
                    bad_names.append(display_name)
                elif any(kw in bad_for for kw in ["sensitive", "ë¯¼ê°ì„±", "acne", "ì—¬ë“œë¦„"]):
                    if display_name not in bad_names:
                        short_desc = description[:100] + "..." if len(description) > 100 else description
                        bad_matches.append({
                            "name": display_name,
                            "description": short_desc if short_desc else "ì¼ë¶€ í”¼ë¶€ì— ìê·¹ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                        })
                        bad_names.append(display_name)
            
            # ì„±ë¶„ ëª©ì  ì§‘ê³„
            from collections import Counter
            all_purposes = []
            for info in ingredient_info_map.values():
                purpose = info.get('purpose', []) or []
                if isinstance(purpose, list):
                    all_purposes.extend(purpose)
                elif isinstance(purpose, str):
                    all_purposes.extend([p.strip() for p in purpose.split(',') if p.strip()])
            
            purpose_counts = Counter(all_purposes)
            common_purposes_str = ", ".join([f"{p} ({c}íšŒ)" for p, c in purpose_counts.most_common(3)])
            
            # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
            analysis_prompt = f"""ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…: {skin_type}
ì¢‹ì€ ì„±ë¶„ ëª©ë¡: {', '.join(good_names) if good_names else 'ì—†ìŒ'}
ì£¼ì˜ ì„±ë¶„ ëª©ë¡ (ì¼ë°˜ì  í¬í•¨): {', '.join(bad_names) if bad_names else 'ì—†ìŒ'}
ì°¸ê³ ìš© (ì£¼ìš” ì„±ë¶„ ëª©ì ): {common_purposes_str}
"""
            
            analysis_report = self.llm.invoke(analysis_prompt)
            
            return {
                "analysis_report": analysis_report,
                "good_matches": good_matches,
                "bad_matches": bad_matches,
                "success": True
            }
            
        except Exception as e:
            return {
                "analysis_report": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "good_matches": [],
                "bad_matches": [],
                "success": False
            }
    
    def _get_ingredients_from_local(self, names: List[str]) -> Dict[str, Dict]:
        """
        ë¡œì»¬ ë°ì´í„°(JSON)ì—ì„œ ì„±ë¶„ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Supabaseë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ í´ë°±ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        
        ê²€ìƒ‰ ë°©ì‹:
        1. ì •í™• ë§¤ì¹­: í•œêµ­ì–´ ì´ë¦„ ë˜ëŠ” ì˜ì–´ ì´ë¦„ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜
        2. ë¶€ë¶„ ë§¤ì¹­: ì •í™• ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ë¶€ë¶„ ë¬¸ìì—´ë¡œ ê²€ìƒ‰
        
        ì •ê·œí™”:
        - ê³µë°± ì œê±°
        - ì†Œë¬¸ì ë³€í™˜
        
        Args:
            names: ê²€ìƒ‰í•  ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì„±ë¶„ëª… â†’ ì„±ë¶„ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë§¤í•‘
        """
        result_map = {}
        
        kor_index = {item['kor_name'].lower().replace(" ", ""): item 
                     for item in self.ingredients_data if item.get('kor_name')}
        eng_index = {item['eng_name'].lower().replace(" ", ""): item 
                     for item in self.ingredients_data if item.get('eng_name')}
        
        for name in names:
            normalized = name.strip().lower().replace(" ", "")
            
            if normalized in kor_index:
                result_map[name] = kor_index[normalized]
            elif normalized in eng_index:
                result_map[name] = eng_index[normalized]
            else:
                # ë¶€ë¶„ ë§¤ì¹­
                for kor_name, item in kor_index.items():
                    if normalized in kor_name or kor_name in normalized:
                        result_map[name] = item
                        break
        
        return result_map


# ============================================================================
# FastAPI ì•±
# ============================================================================

app = FastAPI(
    title="í™”ì¥í’ˆ ì„±ë¶„ RAG API (Supabase)",
    description="PostgreSQL + ChromaDB í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ",
    version="3.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
logger.info("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
ingredients_file = os.path.join(project_root, 'app', 'src', 'main', 'assets', 'ingredients.json')
rag_system = EnterpriseRAG(ingredients_file)


@app.get("/", tags=["Root"])
async def root():
    """
    ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
    
    API ì„œë²„ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        ì„œë²„ ì •ë³´ ë”•ì…”ë„ˆë¦¬:
        - message: ì„œë²„ ë©”ì‹œì§€
        - version: API ë²„ì „
        - database: ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°ë² ì´ìŠ¤
        - docs: API ë¬¸ì„œ URL
    """
    return {
        "message": "í™”ì¥í’ˆ ì„±ë¶„ RAG API ì„œë²„ (Supabase)",
        "version": "3.0.0",
        "database": rag_system.get_data_source(),
        "docs": "/docs"
    }


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """
    ì„œë²„ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
    
    ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì¸ì§€ í™•ì¸í•˜ê³  í˜„ì¬ ìƒíƒœ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        HealthResponse: ì„œë²„ ìƒíƒœ ì •ë³´
            - status: "healthy"
            - message: ìƒíƒœ ë©”ì‹œì§€
            - ingredients_count: ì €ì¥ëœ ì„±ë¶„ ê°œìˆ˜
            - database: ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°ë² ì´ìŠ¤
            - features: ì§€ì›í•˜ëŠ” ê¸°ëŠ¥ ë¦¬ìŠ¤íŠ¸
    """
    return HealthResponse(
        status="healthy",
        message="RAG ì„œë²„ ì •ìƒ ì‘ë™ ì¤‘",
        ingredients_count=rag_system.get_ingredients_count(),
        database=rag_system.get_data_source(),
        features=[
            "Supabase PostgreSQL" if rag_system.use_supabase else "JSON Fallback",
            "ChromaDB Vector Store",
            "LangChain RAG Pipeline",
            "FastAPI Async"
        ]
    )


@app.post("/search", response_model=SearchResponse, tags=["Search"])
async def search_ingredients(request: SearchRequest):
    """
    ì„±ë¶„ ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸
    
    ì„±ë¶„ëª… ë˜ëŠ” ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„ ê´€ë ¨ ì„±ë¶„ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        request: ê²€ìƒ‰ ìš”ì²­ (SearchRequest)
            - query: ê²€ìƒ‰í•  ì„±ë¶„ëª… ë˜ëŠ” ì§ˆë¬¸
            - session_id: ì±„íŒ… ì„¸ì…˜ ID (ì„ íƒì )
    
    Returns:
        SearchResponse: ê²€ìƒ‰ ê²°ê³¼
            - query: ê²€ìƒ‰ ì¿¼ë¦¬
            - answer: ê²€ìƒ‰ ê²°ê³¼ ë‹µë³€
            - similar_ingredients: ìœ ì‚¬í•œ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
            - session_id: ì„¸ì…˜ ID
            - chat_history: ì±„íŒ… íˆìŠ¤í† ë¦¬
            - success: ì„±ê³µ ì—¬ë¶€
    
    Raises:
        HTTPException: ê²€ìƒ‰ì–´ê°€ ë¹„ì–´ìˆì„ ê²½ìš° 400 ì—ëŸ¬
    """
    if not request.query:
        raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
    
    result = rag_system.search_ingredients(request.query, request.session_id)
    return SearchResponse(**result)


@app.post("/analyze_product", response_model=AnalyzeProductResponse, tags=["Analysis"])
async def analyze_product(request: AnalyzeProductRequest):
    """
    ì œí’ˆ ì„±ë¶„ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸
    
    í™”ì¥í’ˆì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì í”¼ë¶€ íƒ€ì…ì— ë§ëŠ” í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    ì²˜ë¦¬ ê³¼ì •:
    1. ì„±ë¶„ ì •ë³´ ì¡°íšŒ (Supabase ë˜ëŠ” ë¡œì»¬)
    2. ê° ì„±ë¶„ì˜ good_for/bad_for í™•ì¸
    3. ì¢‹ì€ ì„±ë¶„/ì£¼ì˜ ì„±ë¶„ ë¶„ë¥˜
    4. ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (MockLLM)
    
    Args:
        request: ë¶„ì„ ìš”ì²­ (AnalyzeProductRequest)
            - ingredients: ë¶„ì„í•  ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸
            - skin_type: ì‚¬ìš©ì í”¼ë¶€ íƒ€ì… (ì˜ˆ: "ê±´ì„±, ë¯¼ê°ì„±")
    
    Returns:
        AnalyzeProductResponse: ë¶„ì„ ê²°ê³¼
            - analysis_report: ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ (í•œêµ­ì–´)
            - good_matches: ì¢‹ì€ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
            - bad_matches: ì£¼ì˜ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸
            - success: ë¶„ì„ ì„±ê³µ ì—¬ë¶€
    
    Raises:
        HTTPException: ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì„ ê²½ìš° 400 ì—ëŸ¬
    """
    if not request.ingredients:
        raise HTTPException(status_code=400, detail="ì„±ë¶„ ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    result = rag_system.analyze_product_ingredients(request.ingredients, request.skin_type)
    
    good_matches = [GoodMatch(**m) for m in result["good_matches"]]
    bad_matches = [BadMatch(**m) for m in result["bad_matches"]]
    
    return AnalyzeProductResponse(
        analysis_report=result["analysis_report"],
        good_matches=good_matches,
        bad_matches=bad_matches,
        success=result["success"]
    )


@app.get("/ingredients", tags=["Ingredients"])
async def get_all_ingredients_api():
    """
    ëª¨ë“  ì„±ë¶„ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
    
    ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ì„±ë¶„ì˜ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    í•œêµ­ì–´ ì´ë¦„ê³¼ ì˜ì–´ ì´ë¦„ì´ ëª¨ë‘ ìˆìœ¼ë©´ "í•œêµ­ì–´ëª… (ì˜ì–´ëª…)" í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        ì„±ë¶„ ëª©ë¡ ë”•ì…”ë„ˆë¦¬:
        - ingredients: ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸
        - count: ì„±ë¶„ ê°œìˆ˜
        - database: ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°ë² ì´ìŠ¤
        - success: ì„±ê³µ ì—¬ë¶€
    """
    if rag_system.use_supabase:
        ingredients = get_all_ingredients()
    else:
        ingredients = rag_system.ingredients_data
    
    result = []
    for item in ingredients:
        kor = item.get('kor_name', '')
        eng = item.get('eng_name', '')
        if kor and eng:
            result.append(f"{kor} ({eng})")
        elif kor:
            result.append(kor)
        elif eng:
            result.append(eng)
    
    return {
        'ingredients': result,
        'count': len(result),
        'database': rag_system.get_data_source(),
        'success': True
    }


@app.get("/database/status", tags=["Database"])
async def database_status():
    """ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸"""
    if rag_system.use_supabase:
        test_result = test_supabase_connection()
        return {
            "database": "supabase",
            "connected": test_result["success"],
            "message": test_result["message"],
            "ingredients_count": rag_system.get_ingredients_count()
        }
    else:
        return {
            "database": "json",
            "connected": True,
            "message": "JSON íŒŒì¼ ëª¨ë“œ (Supabase ì—°ê²° ì•ˆë¨)",
            "ingredients_count": len(rag_system.ingredients_data)
        }


if __name__ == '__main__':
    import uvicorn
    
    logger.info("=" * 60)
    logger.info("ğŸš€ í™”ì¥í’ˆ ì„±ë¶„ RAG ì„œë²„ (Supabase ë²„ì „)")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š ë°ì´í„° ì†ŒìŠ¤: {rag_system.get_data_source()}")
    logger.info(f"ğŸ“¦ ì„±ë¶„ ê°œìˆ˜: {rag_system.get_ingredients_count()}")
    logger.info("ğŸ“š API ë¬¸ì„œ: http://localhost:5000/docs")
    logger.info("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=5000, log_level="info")

